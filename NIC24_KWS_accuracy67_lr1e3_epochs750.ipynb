{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xkd5EABaGJFY"
      },
      "outputs": [],
      "source": [
        "!pip install tonic --quiet\n",
        "!pip install snntorch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SjmSndG5Fgnf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import tonic\n",
        "import snntorch as snn\n",
        "from snntorch import functional as SF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, trainloader, device, num_epochs=30):\n",
        "  for epoch in range(num_epochs):\n",
        "      # Training loop\n",
        "      model.train()\n",
        "      for batch_idx, (data, target) in enumerate(trainloader):\n",
        "          optimizer.zero_grad()\n",
        "          spk_rec, mem_rec = model(data.to(device))\n",
        "          loss = criterion(spk_rec, target.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % 100 == 0:\n",
        "              print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "def accuracy(model, testloader, device):\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        correct2 = 0\n",
        "        total2 = 0\n",
        "        for data, target in testloader:\n",
        "            spk_rec, mem_rec = model(data.to(device))\n",
        "            print('input: ', data.size(), ' output: ', spk_rec.size())\n",
        "            correct += SF.accuracy_rate(spk_rec, target.to(device)) * target.size(0)\n",
        "            total += spk_rec.size(1)\n",
        "            total2 += target.size(0)\n",
        "            print(correct, ' out of ', target.size())\n",
        "\n",
        "            # correct2 += (predicted == target).sum().item()\n",
        "\n",
        "        print(f'Accuracy of the network on test set: {100 * correct / total:.2f}% ({100 * correct / total2:.2f}%)')"
      ],
      "metadata": {
        "id": "PFC7N2U8eBpn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "7kC-Q0tZGNiv"
      },
      "outputs": [],
      "source": [
        "from tonic import datasets, transforms\n",
        "\n",
        "dt = 100000\n",
        "\n",
        "# encoding_dim = 100\n",
        "\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                # transforms.Downsample(spatial_factor=encoding_dim/700),\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                #transforms.Downsample(spatial_factor=0.057),\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    # sensor_size=(40,1,1),\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "kWCnrwvnGPqy"
      },
      "outputs": [],
      "source": [
        "from tonic import DiskCachedDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 4096\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "\n",
        "\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzqvT4zYGR-c",
        "outputId": "2c8ed505-3a22-41e5-a1e4-1a383a7f8f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 4096, 1, 700])\n",
            "tensor([16, 19, 14,  ..., 16, 16,  6])\n"
          ]
        }
      ],
      "source": [
        "for data, labels in iter(trainloader):\n",
        "  print(data.size())\n",
        "  print(labels)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zEbJ2rqSFkYy"
      },
      "outputs": [],
      "source": [
        "# Define the fully connected network\n",
        "class FullyConnectedNetwork(nn.Module):\n",
        "    def __init__(self, beta=0.95):\n",
        "        super(FullyConnectedNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(700, 256)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, 20)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "      spk_rec = []\n",
        "      mem_rec = []\n",
        "\n",
        "      mem1 = self.lif1.init_leaky()\n",
        "      mem2 = self.lif2.init_leaky()\n",
        "\n",
        "      for step in range(x.size(0)):\n",
        "        cur1 = self.fc1(x[step].squeeze(1))\n",
        "        spk1, mem1 = self.lif1(cur1, mem1)\n",
        "        cur2 = self.fc2(spk1)\n",
        "        spk2, mem2 = self.lif2(cur2, mem2)\n",
        "        spk_rec.append(spk2)\n",
        "        mem_rec.append(mem2)\n",
        "      return torch.stack(spk_rec), torch.stack(mem_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "jjIM6Ur9FLDN"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork()\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# # Check if CUDA is available\n",
        "# if torch.cuda.is_available():\n",
        "#     print(f\"CUDA is available. Number of devices: {torch.cuda.device_count()}\")\n",
        "\n",
        "#     for i in range(torch.cuda.device_count()):\n",
        "#         print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "#         print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i)} bytes\")\n",
        "#         print(f\"  Memory Cached: {torch.cuda.memory_reserved(i)} bytes\")\n",
        "#         print(f\"  Memory Free: {torch.cuda.get_device_properties(i).total_memory - torch.cuda.memory_allocated(i)} bytes\")\n",
        "# else:\n",
        "#     print(\"CUDA is not available.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "FtC5R5NyT-qT"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCOqTOCmqa1K",
        "outputId": "64ee9ce9-f61e-439d-c762-3717620d7b00"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1/2], Loss: 3.0120\n",
            "Epoch [2/100], Step [1/2], Loss: 2.9849\n",
            "Epoch [3/100], Step [1/2], Loss: 2.9618\n",
            "Epoch [4/100], Step [1/2], Loss: 2.9498\n",
            "Epoch [5/100], Step [1/2], Loss: 2.9286\n",
            "Epoch [6/100], Step [1/2], Loss: 2.9081\n",
            "Epoch [7/100], Step [1/2], Loss: 2.8873\n",
            "Epoch [8/100], Step [1/2], Loss: 2.8729\n",
            "Epoch [9/100], Step [1/2], Loss: 2.8575\n",
            "Epoch [10/100], Step [1/2], Loss: 2.8337\n",
            "Epoch [11/100], Step [1/2], Loss: 2.8106\n",
            "Epoch [12/100], Step [1/2], Loss: 2.7967\n",
            "Epoch [13/100], Step [1/2], Loss: 2.7760\n",
            "Epoch [14/100], Step [1/2], Loss: 2.7457\n",
            "Epoch [15/100], Step [1/2], Loss: 2.7274\n",
            "Epoch [16/100], Step [1/2], Loss: 2.7148\n",
            "Epoch [17/100], Step [1/2], Loss: 2.6933\n",
            "Epoch [18/100], Step [1/2], Loss: 2.6818\n",
            "Epoch [19/100], Step [1/2], Loss: 2.6596\n",
            "Epoch [20/100], Step [1/2], Loss: 2.6427\n",
            "Epoch [21/100], Step [1/2], Loss: 2.6281\n",
            "Epoch [22/100], Step [1/2], Loss: 2.6185\n",
            "Epoch [23/100], Step [1/2], Loss: 2.6035\n",
            "Epoch [24/100], Step [1/2], Loss: 2.5890\n",
            "Epoch [25/100], Step [1/2], Loss: 2.5820\n",
            "Epoch [26/100], Step [1/2], Loss: 2.5711\n",
            "Epoch [27/100], Step [1/2], Loss: 2.5619\n",
            "Epoch [28/100], Step [1/2], Loss: 2.5568\n",
            "Epoch [29/100], Step [1/2], Loss: 2.5490\n",
            "Epoch [30/100], Step [1/2], Loss: 2.5364\n",
            "Epoch [31/100], Step [1/2], Loss: 2.5350\n",
            "Epoch [32/100], Step [1/2], Loss: 2.5271\n",
            "Epoch [33/100], Step [1/2], Loss: 2.5211\n",
            "Epoch [34/100], Step [1/2], Loss: 2.5126\n",
            "Epoch [35/100], Step [1/2], Loss: 2.5135\n",
            "Epoch [36/100], Step [1/2], Loss: 2.5075\n",
            "Epoch [37/100], Step [1/2], Loss: 2.4988\n",
            "Epoch [38/100], Step [1/2], Loss: 2.4940\n",
            "Epoch [39/100], Step [1/2], Loss: 2.4957\n",
            "Epoch [40/100], Step [1/2], Loss: 2.4909\n",
            "Epoch [41/100], Step [1/2], Loss: 2.4798\n",
            "Epoch [42/100], Step [1/2], Loss: 2.4769\n",
            "Epoch [43/100], Step [1/2], Loss: 2.4768\n",
            "Epoch [44/100], Step [1/2], Loss: 2.4736\n",
            "Epoch [45/100], Step [1/2], Loss: 2.4657\n",
            "Epoch [46/100], Step [1/2], Loss: 2.4630\n",
            "Epoch [47/100], Step [1/2], Loss: 2.4566\n",
            "Epoch [48/100], Step [1/2], Loss: 2.4607\n",
            "Epoch [49/100], Step [1/2], Loss: 2.4528\n",
            "Epoch [50/100], Step [1/2], Loss: 2.4493\n",
            "Epoch [51/100], Step [1/2], Loss: 2.4459\n",
            "Epoch [52/100], Step [1/2], Loss: 2.4413\n",
            "Epoch [53/100], Step [1/2], Loss: 2.4434\n",
            "Epoch [54/100], Step [1/2], Loss: 2.4393\n",
            "Epoch [55/100], Step [1/2], Loss: 2.4336\n",
            "Epoch [56/100], Step [1/2], Loss: 2.4322\n",
            "Epoch [57/100], Step [1/2], Loss: 2.4306\n",
            "Epoch [58/100], Step [1/2], Loss: 2.4322\n",
            "Epoch [59/100], Step [1/2], Loss: 2.4256\n",
            "Epoch [60/100], Step [1/2], Loss: 2.4250\n",
            "Epoch [61/100], Step [1/2], Loss: 2.4206\n",
            "Epoch [62/100], Step [1/2], Loss: 2.4185\n",
            "Epoch [63/100], Step [1/2], Loss: 2.4158\n",
            "Epoch [64/100], Step [1/2], Loss: 2.4135\n",
            "Epoch [65/100], Step [1/2], Loss: 2.4136\n",
            "Epoch [66/100], Step [1/2], Loss: 2.4068\n",
            "Epoch [67/100], Step [1/2], Loss: 2.4054\n",
            "Epoch [68/100], Step [1/2], Loss: 2.4038\n",
            "Epoch [69/100], Step [1/2], Loss: 2.4014\n",
            "Epoch [70/100], Step [1/2], Loss: 2.4040\n",
            "Epoch [71/100], Step [1/2], Loss: 2.3970\n",
            "Epoch [72/100], Step [1/2], Loss: 2.3953\n",
            "Epoch [73/100], Step [1/2], Loss: 2.3936\n",
            "Epoch [74/100], Step [1/2], Loss: 2.3896\n",
            "Epoch [75/100], Step [1/2], Loss: 2.3909\n",
            "Epoch [76/100], Step [1/2], Loss: 2.3883\n",
            "Epoch [77/100], Step [1/2], Loss: 2.3847\n",
            "Epoch [78/100], Step [1/2], Loss: 2.3807\n",
            "Epoch [79/100], Step [1/2], Loss: 2.3819\n",
            "Epoch [80/100], Step [1/2], Loss: 2.3802\n",
            "Epoch [81/100], Step [1/2], Loss: 2.3743\n",
            "Epoch [82/100], Step [1/2], Loss: 2.3803\n",
            "Epoch [83/100], Step [1/2], Loss: 2.3749\n",
            "Epoch [84/100], Step [1/2], Loss: 2.3725\n",
            "Epoch [85/100], Step [1/2], Loss: 2.3713\n",
            "Epoch [86/100], Step [1/2], Loss: 2.3709\n",
            "Epoch [87/100], Step [1/2], Loss: 2.3659\n",
            "Epoch [88/100], Step [1/2], Loss: 2.3664\n",
            "Epoch [89/100], Step [1/2], Loss: 2.3659\n",
            "Epoch [90/100], Step [1/2], Loss: 2.3615\n",
            "Epoch [91/100], Step [1/2], Loss: 2.3618\n",
            "Epoch [92/100], Step [1/2], Loss: 2.3607\n",
            "Epoch [93/100], Step [1/2], Loss: 2.3591\n",
            "Epoch [94/100], Step [1/2], Loss: 2.3553\n",
            "Epoch [95/100], Step [1/2], Loss: 2.3548\n",
            "Epoch [96/100], Step [1/2], Loss: 2.3530\n",
            "Epoch [97/100], Step [1/2], Loss: 2.3523\n",
            "Epoch [98/100], Step [1/2], Loss: 2.3490\n",
            "Epoch [99/100], Step [1/2], Loss: 2.3492\n",
            "Epoch [100/100], Step [1/2], Loss: 2.3453\n",
            "input:  torch.Size([10, 2264, 1, 700])  output:  torch.Size([10, 2264, 20])\n",
            "1225.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 54.11% (54.11%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1c_zXV6u4to",
        "outputId": "429947d3-859a-4cdd-cd0b-32752649f96e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1/2], Loss: 3.0349\n",
            "Epoch [2/100], Step [1/2], Loss: 2.9761\n",
            "Epoch [3/100], Step [1/2], Loss: 2.9439\n",
            "Epoch [4/100], Step [1/2], Loss: 2.9132\n",
            "Epoch [5/100], Step [1/2], Loss: 2.8830\n",
            "Epoch [6/100], Step [1/2], Loss: 2.8586\n",
            "Epoch [7/100], Step [1/2], Loss: 2.8203\n",
            "Epoch [8/100], Step [1/2], Loss: 2.7884\n",
            "Epoch [9/100], Step [1/2], Loss: 2.7602\n",
            "Epoch [10/100], Step [1/2], Loss: 2.7312\n",
            "Epoch [11/100], Step [1/2], Loss: 2.6985\n",
            "Epoch [12/100], Step [1/2], Loss: 2.6747\n",
            "Epoch [13/100], Step [1/2], Loss: 2.6479\n",
            "Epoch [14/100], Step [1/2], Loss: 2.6308\n",
            "Epoch [15/100], Step [1/2], Loss: 2.6102\n",
            "Epoch [16/100], Step [1/2], Loss: 2.5905\n",
            "Epoch [17/100], Step [1/2], Loss: 2.5776\n",
            "Epoch [18/100], Step [1/2], Loss: 2.5639\n",
            "Epoch [19/100], Step [1/2], Loss: 2.5457\n",
            "Epoch [20/100], Step [1/2], Loss: 2.5390\n",
            "Epoch [21/100], Step [1/2], Loss: 2.5266\n",
            "Epoch [22/100], Step [1/2], Loss: 2.5201\n",
            "Epoch [23/100], Step [1/2], Loss: 2.5105\n",
            "Epoch [24/100], Step [1/2], Loss: 2.4998\n",
            "Epoch [25/100], Step [1/2], Loss: 2.4971\n",
            "Epoch [26/100], Step [1/2], Loss: 2.4830\n",
            "Epoch [27/100], Step [1/2], Loss: 2.4797\n",
            "Epoch [28/100], Step [1/2], Loss: 2.4727\n",
            "Epoch [29/100], Step [1/2], Loss: 2.4702\n",
            "Epoch [30/100], Step [1/2], Loss: 2.4625\n",
            "Epoch [31/100], Step [1/2], Loss: 2.4587\n",
            "Epoch [32/100], Step [1/2], Loss: 2.4467\n",
            "Epoch [33/100], Step [1/2], Loss: 2.4469\n",
            "Epoch [34/100], Step [1/2], Loss: 2.4431\n",
            "Epoch [35/100], Step [1/2], Loss: 2.4360\n",
            "Epoch [36/100], Step [1/2], Loss: 2.4352\n",
            "Epoch [37/100], Step [1/2], Loss: 2.4307\n",
            "Epoch [38/100], Step [1/2], Loss: 2.4285\n",
            "Epoch [39/100], Step [1/2], Loss: 2.4237\n",
            "Epoch [40/100], Step [1/2], Loss: 2.4185\n",
            "Epoch [41/100], Step [1/2], Loss: 2.4147\n",
            "Epoch [42/100], Step [1/2], Loss: 2.4112\n",
            "Epoch [43/100], Step [1/2], Loss: 2.4054\n",
            "Epoch [44/100], Step [1/2], Loss: 2.4028\n",
            "Epoch [45/100], Step [1/2], Loss: 2.3976\n",
            "Epoch [46/100], Step [1/2], Loss: 2.3969\n",
            "Epoch [47/100], Step [1/2], Loss: 2.3925\n",
            "Epoch [48/100], Step [1/2], Loss: 2.3895\n",
            "Epoch [49/100], Step [1/2], Loss: 2.3886\n",
            "Epoch [50/100], Step [1/2], Loss: 2.3834\n",
            "Epoch [51/100], Step [1/2], Loss: 2.3839\n",
            "Epoch [52/100], Step [1/2], Loss: 2.3803\n",
            "Epoch [53/100], Step [1/2], Loss: 2.3758\n",
            "Epoch [54/100], Step [1/2], Loss: 2.3731\n",
            "Epoch [55/100], Step [1/2], Loss: 2.3713\n",
            "Epoch [56/100], Step [1/2], Loss: 2.3695\n",
            "Epoch [57/100], Step [1/2], Loss: 2.3664\n",
            "Epoch [58/100], Step [1/2], Loss: 2.3658\n",
            "Epoch [59/100], Step [1/2], Loss: 2.3627\n",
            "Epoch [60/100], Step [1/2], Loss: 2.3612\n",
            "Epoch [61/100], Step [1/2], Loss: 2.3595\n",
            "Epoch [62/100], Step [1/2], Loss: 2.3550\n",
            "Epoch [63/100], Step [1/2], Loss: 2.3530\n",
            "Epoch [64/100], Step [1/2], Loss: 2.3528\n",
            "Epoch [65/100], Step [1/2], Loss: 2.3499\n",
            "Epoch [66/100], Step [1/2], Loss: 2.3442\n",
            "Epoch [67/100], Step [1/2], Loss: 2.3422\n",
            "Epoch [68/100], Step [1/2], Loss: 2.3481\n",
            "Epoch [69/100], Step [1/2], Loss: 2.3401\n",
            "Epoch [70/100], Step [1/2], Loss: 2.3360\n",
            "Epoch [71/100], Step [1/2], Loss: 2.3360\n",
            "Epoch [72/100], Step [1/2], Loss: 2.3345\n",
            "Epoch [73/100], Step [1/2], Loss: 2.3321\n",
            "Epoch [74/100], Step [1/2], Loss: 2.3281\n",
            "Epoch [75/100], Step [1/2], Loss: 2.3292\n",
            "Epoch [76/100], Step [1/2], Loss: 2.3251\n",
            "Epoch [77/100], Step [1/2], Loss: 2.3238\n",
            "Epoch [78/100], Step [1/2], Loss: 2.3220\n",
            "Epoch [79/100], Step [1/2], Loss: 2.3219\n",
            "Epoch [80/100], Step [1/2], Loss: 2.3188\n",
            "Epoch [81/100], Step [1/2], Loss: 2.3198\n",
            "Epoch [82/100], Step [1/2], Loss: 2.3132\n",
            "Epoch [83/100], Step [1/2], Loss: 2.3141\n",
            "Epoch [84/100], Step [1/2], Loss: 2.3101\n",
            "Epoch [85/100], Step [1/2], Loss: 2.3116\n",
            "Epoch [86/100], Step [1/2], Loss: 2.3122\n",
            "Epoch [87/100], Step [1/2], Loss: 2.3082\n",
            "Epoch [88/100], Step [1/2], Loss: 2.3070\n",
            "Epoch [89/100], Step [1/2], Loss: 2.3057\n",
            "Epoch [90/100], Step [1/2], Loss: 2.3026\n",
            "Epoch [91/100], Step [1/2], Loss: 2.3037\n",
            "Epoch [92/100], Step [1/2], Loss: 2.3002\n",
            "Epoch [93/100], Step [1/2], Loss: 2.3014\n",
            "Epoch [94/100], Step [1/2], Loss: 2.2941\n",
            "Epoch [95/100], Step [1/2], Loss: 2.2964\n",
            "Epoch [96/100], Step [1/2], Loss: 2.2958\n",
            "Epoch [97/100], Step [1/2], Loss: 2.2958\n",
            "Epoch [98/100], Step [1/2], Loss: 2.2962\n",
            "Epoch [99/100], Step [1/2], Loss: 2.2927\n",
            "Epoch [100/100], Step [1/2], Loss: 2.2891\n",
            "input:  torch.Size([10, 2264, 1, 700])  output:  torch.Size([10, 2264, 20])\n",
            "1339.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 59.14% (59.14%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.5).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFNSLXS5u7wH",
        "outputId": "58d74f25-5cce-49a9-c69c-a39dce849073"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1/1], Loss: 2.9981\n",
            "Epoch [2/100], Step [1/1], Loss: 2.9983\n",
            "Epoch [3/100], Step [1/1], Loss: 2.9980\n",
            "Epoch [4/100], Step [1/1], Loss: 2.9943\n",
            "Epoch [5/100], Step [1/1], Loss: 2.9913\n",
            "Epoch [6/100], Step [1/1], Loss: 2.9879\n",
            "Epoch [7/100], Step [1/1], Loss: 2.9837\n",
            "Epoch [8/100], Step [1/1], Loss: 2.9790\n",
            "Epoch [9/100], Step [1/1], Loss: 2.9751\n",
            "Epoch [10/100], Step [1/1], Loss: 2.9721\n",
            "Epoch [11/100], Step [1/1], Loss: 2.9688\n",
            "Epoch [12/100], Step [1/1], Loss: 2.9660\n",
            "Epoch [13/100], Step [1/1], Loss: 2.9621\n",
            "Epoch [14/100], Step [1/1], Loss: 2.9581\n",
            "Epoch [15/100], Step [1/1], Loss: 2.9541\n",
            "Epoch [16/100], Step [1/1], Loss: 2.9493\n",
            "Epoch [17/100], Step [1/1], Loss: 2.9451\n",
            "Epoch [18/100], Step [1/1], Loss: 2.9403\n",
            "Epoch [19/100], Step [1/1], Loss: 2.9367\n",
            "Epoch [20/100], Step [1/1], Loss: 2.9332\n",
            "Epoch [21/100], Step [1/1], Loss: 2.9298\n",
            "Epoch [22/100], Step [1/1], Loss: 2.9256\n",
            "Epoch [23/100], Step [1/1], Loss: 2.9221\n",
            "Epoch [24/100], Step [1/1], Loss: 2.9187\n",
            "Epoch [25/100], Step [1/1], Loss: 2.9150\n",
            "Epoch [26/100], Step [1/1], Loss: 2.9114\n",
            "Epoch [27/100], Step [1/1], Loss: 2.9082\n",
            "Epoch [28/100], Step [1/1], Loss: 2.9054\n",
            "Epoch [29/100], Step [1/1], Loss: 2.9022\n",
            "Epoch [30/100], Step [1/1], Loss: 2.8985\n",
            "Epoch [31/100], Step [1/1], Loss: 2.8953\n",
            "Epoch [32/100], Step [1/1], Loss: 2.8920\n",
            "Epoch [33/100], Step [1/1], Loss: 2.8889\n",
            "Epoch [34/100], Step [1/1], Loss: 2.8856\n",
            "Epoch [35/100], Step [1/1], Loss: 2.8827\n",
            "Epoch [36/100], Step [1/1], Loss: 2.8802\n",
            "Epoch [37/100], Step [1/1], Loss: 2.8775\n",
            "Epoch [38/100], Step [1/1], Loss: 2.8751\n",
            "Epoch [39/100], Step [1/1], Loss: 2.8730\n",
            "Epoch [40/100], Step [1/1], Loss: 2.8706\n",
            "Epoch [41/100], Step [1/1], Loss: 2.8686\n",
            "Epoch [42/100], Step [1/1], Loss: 2.8669\n",
            "Epoch [43/100], Step [1/1], Loss: 2.8645\n",
            "Epoch [44/100], Step [1/1], Loss: 2.8629\n",
            "Epoch [45/100], Step [1/1], Loss: 2.8610\n",
            "Epoch [46/100], Step [1/1], Loss: 2.8589\n",
            "Epoch [47/100], Step [1/1], Loss: 2.8573\n",
            "Epoch [48/100], Step [1/1], Loss: 2.8559\n",
            "Epoch [49/100], Step [1/1], Loss: 2.8540\n",
            "Epoch [50/100], Step [1/1], Loss: 2.8523\n",
            "Epoch [51/100], Step [1/1], Loss: 2.8508\n",
            "Epoch [52/100], Step [1/1], Loss: 2.8494\n",
            "Epoch [53/100], Step [1/1], Loss: 2.8480\n",
            "Epoch [54/100], Step [1/1], Loss: 2.8467\n",
            "Epoch [55/100], Step [1/1], Loss: 2.8448\n",
            "Epoch [56/100], Step [1/1], Loss: 2.8432\n",
            "Epoch [57/100], Step [1/1], Loss: 2.8418\n",
            "Epoch [58/100], Step [1/1], Loss: 2.8403\n",
            "Epoch [59/100], Step [1/1], Loss: 2.8389\n",
            "Epoch [60/100], Step [1/1], Loss: 2.8376\n",
            "Epoch [61/100], Step [1/1], Loss: 2.8362\n",
            "Epoch [62/100], Step [1/1], Loss: 2.8350\n",
            "Epoch [63/100], Step [1/1], Loss: 2.8332\n",
            "Epoch [64/100], Step [1/1], Loss: 2.8317\n",
            "Epoch [65/100], Step [1/1], Loss: 2.8304\n",
            "Epoch [66/100], Step [1/1], Loss: 2.8288\n",
            "Epoch [67/100], Step [1/1], Loss: 2.8278\n",
            "Epoch [68/100], Step [1/1], Loss: 2.8261\n",
            "Epoch [69/100], Step [1/1], Loss: 2.8248\n",
            "Epoch [70/100], Step [1/1], Loss: 2.8236\n",
            "Epoch [71/100], Step [1/1], Loss: 2.8221\n",
            "Epoch [72/100], Step [1/1], Loss: 2.8208\n",
            "Epoch [73/100], Step [1/1], Loss: 2.8196\n",
            "Epoch [74/100], Step [1/1], Loss: 2.8184\n",
            "Epoch [75/100], Step [1/1], Loss: 2.8174\n",
            "Epoch [76/100], Step [1/1], Loss: 2.8164\n",
            "Epoch [77/100], Step [1/1], Loss: 2.8151\n",
            "Epoch [78/100], Step [1/1], Loss: 2.8138\n",
            "Epoch [79/100], Step [1/1], Loss: 2.8127\n",
            "Epoch [80/100], Step [1/1], Loss: 2.8115\n",
            "Epoch [81/100], Step [1/1], Loss: 2.8106\n",
            "Epoch [82/100], Step [1/1], Loss: 2.8095\n",
            "Epoch [83/100], Step [1/1], Loss: 2.8084\n",
            "Epoch [84/100], Step [1/1], Loss: 2.8070\n",
            "Epoch [85/100], Step [1/1], Loss: 2.8057\n",
            "Epoch [86/100], Step [1/1], Loss: 2.8045\n",
            "Epoch [87/100], Step [1/1], Loss: 2.8034\n",
            "Epoch [88/100], Step [1/1], Loss: 2.8020\n",
            "Epoch [89/100], Step [1/1], Loss: 2.8010\n",
            "Epoch [90/100], Step [1/1], Loss: 2.7998\n",
            "Epoch [91/100], Step [1/1], Loss: 2.7987\n",
            "Epoch [92/100], Step [1/1], Loss: 2.7979\n",
            "Epoch [93/100], Step [1/1], Loss: 2.7965\n",
            "Epoch [94/100], Step [1/1], Loss: 2.7955\n",
            "Epoch [95/100], Step [1/1], Loss: 2.7947\n",
            "Epoch [96/100], Step [1/1], Loss: 2.7939\n",
            "Epoch [97/100], Step [1/1], Loss: 2.7933\n",
            "Epoch [98/100], Step [1/1], Loss: 2.7922\n",
            "Epoch [99/100], Step [1/1], Loss: 2.7912\n",
            "Epoch [100/100], Step [1/1], Loss: 2.7901\n",
            "input:  torch.Size([20, 2264, 1, 700])  output:  torch.Size([20, 2264, 20])\n",
            "1172.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 51.77% (51.77%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 750\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdJVAaeCLKH5",
        "outputId": "eb410901-7aa3-4017-eda9-c82bc90fedd9"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/750], Step [1/1], Loss: 3.0402\n",
            "Epoch [2/750], Step [1/1], Loss: 2.9988\n",
            "Epoch [3/750], Step [1/1], Loss: 2.9923\n",
            "Epoch [4/750], Step [1/1], Loss: 2.9835\n",
            "Epoch [5/750], Step [1/1], Loss: 2.9726\n",
            "Epoch [6/750], Step [1/1], Loss: 2.9647\n",
            "Epoch [7/750], Step [1/1], Loss: 2.9553\n",
            "Epoch [8/750], Step [1/1], Loss: 2.9442\n",
            "Epoch [9/750], Step [1/1], Loss: 2.9318\n",
            "Epoch [10/750], Step [1/1], Loss: 2.9198\n",
            "Epoch [11/750], Step [1/1], Loss: 2.9088\n",
            "Epoch [12/750], Step [1/1], Loss: 2.8962\n",
            "Epoch [13/750], Step [1/1], Loss: 2.8820\n",
            "Epoch [14/750], Step [1/1], Loss: 2.8684\n",
            "Epoch [15/750], Step [1/1], Loss: 2.8543\n",
            "Epoch [16/750], Step [1/1], Loss: 2.8384\n",
            "Epoch [17/750], Step [1/1], Loss: 2.8227\n",
            "Epoch [18/750], Step [1/1], Loss: 2.8066\n",
            "Epoch [19/750], Step [1/1], Loss: 2.7912\n",
            "Epoch [20/750], Step [1/1], Loss: 2.7764\n",
            "Epoch [21/750], Step [1/1], Loss: 2.7633\n",
            "Epoch [22/750], Step [1/1], Loss: 2.7502\n",
            "Epoch [23/750], Step [1/1], Loss: 2.7377\n",
            "Epoch [24/750], Step [1/1], Loss: 2.7254\n",
            "Epoch [25/750], Step [1/1], Loss: 2.7152\n",
            "Epoch [26/750], Step [1/1], Loss: 2.7070\n",
            "Epoch [27/750], Step [1/1], Loss: 2.6988\n",
            "Epoch [28/750], Step [1/1], Loss: 2.6899\n",
            "Epoch [29/750], Step [1/1], Loss: 2.6812\n",
            "Epoch [30/750], Step [1/1], Loss: 2.6729\n",
            "Epoch [31/750], Step [1/1], Loss: 2.6641\n",
            "Epoch [32/750], Step [1/1], Loss: 2.6554\n",
            "Epoch [33/750], Step [1/1], Loss: 2.6475\n",
            "Epoch [34/750], Step [1/1], Loss: 2.6389\n",
            "Epoch [35/750], Step [1/1], Loss: 2.6307\n",
            "Epoch [36/750], Step [1/1], Loss: 2.6227\n",
            "Epoch [37/750], Step [1/1], Loss: 2.6155\n",
            "Epoch [38/750], Step [1/1], Loss: 2.6090\n",
            "Epoch [39/750], Step [1/1], Loss: 2.6032\n",
            "Epoch [40/750], Step [1/1], Loss: 2.5969\n",
            "Epoch [41/750], Step [1/1], Loss: 2.5903\n",
            "Epoch [42/750], Step [1/1], Loss: 2.5846\n",
            "Epoch [43/750], Step [1/1], Loss: 2.5796\n",
            "Epoch [44/750], Step [1/1], Loss: 2.5745\n",
            "Epoch [45/750], Step [1/1], Loss: 2.5697\n",
            "Epoch [46/750], Step [1/1], Loss: 2.5652\n",
            "Epoch [47/750], Step [1/1], Loss: 2.5612\n",
            "Epoch [48/750], Step [1/1], Loss: 2.5575\n",
            "Epoch [49/750], Step [1/1], Loss: 2.5538\n",
            "Epoch [50/750], Step [1/1], Loss: 2.5500\n",
            "Epoch [51/750], Step [1/1], Loss: 2.5464\n",
            "Epoch [52/750], Step [1/1], Loss: 2.5429\n",
            "Epoch [53/750], Step [1/1], Loss: 2.5390\n",
            "Epoch [54/750], Step [1/1], Loss: 2.5359\n",
            "Epoch [55/750], Step [1/1], Loss: 2.5329\n",
            "Epoch [56/750], Step [1/1], Loss: 2.5295\n",
            "Epoch [57/750], Step [1/1], Loss: 2.5264\n",
            "Epoch [58/750], Step [1/1], Loss: 2.5240\n",
            "Epoch [59/750], Step [1/1], Loss: 2.5212\n",
            "Epoch [60/750], Step [1/1], Loss: 2.5188\n",
            "Epoch [61/750], Step [1/1], Loss: 2.5161\n",
            "Epoch [62/750], Step [1/1], Loss: 2.5134\n",
            "Epoch [63/750], Step [1/1], Loss: 2.5109\n",
            "Epoch [64/750], Step [1/1], Loss: 2.5082\n",
            "Epoch [65/750], Step [1/1], Loss: 2.5053\n",
            "Epoch [66/750], Step [1/1], Loss: 2.5024\n",
            "Epoch [67/750], Step [1/1], Loss: 2.5000\n",
            "Epoch [68/750], Step [1/1], Loss: 2.4975\n",
            "Epoch [69/750], Step [1/1], Loss: 2.4958\n",
            "Epoch [70/750], Step [1/1], Loss: 2.4937\n",
            "Epoch [71/750], Step [1/1], Loss: 2.4915\n",
            "Epoch [72/750], Step [1/1], Loss: 2.4891\n",
            "Epoch [73/750], Step [1/1], Loss: 2.4868\n",
            "Epoch [74/750], Step [1/1], Loss: 2.4843\n",
            "Epoch [75/750], Step [1/1], Loss: 2.4826\n",
            "Epoch [76/750], Step [1/1], Loss: 2.4804\n",
            "Epoch [77/750], Step [1/1], Loss: 2.4780\n",
            "Epoch [78/750], Step [1/1], Loss: 2.4765\n",
            "Epoch [79/750], Step [1/1], Loss: 2.4744\n",
            "Epoch [80/750], Step [1/1], Loss: 2.4725\n",
            "Epoch [81/750], Step [1/1], Loss: 2.4706\n",
            "Epoch [82/750], Step [1/1], Loss: 2.4687\n",
            "Epoch [83/750], Step [1/1], Loss: 2.4671\n",
            "Epoch [84/750], Step [1/1], Loss: 2.4653\n",
            "Epoch [85/750], Step [1/1], Loss: 2.4635\n",
            "Epoch [86/750], Step [1/1], Loss: 2.4618\n",
            "Epoch [87/750], Step [1/1], Loss: 2.4599\n",
            "Epoch [88/750], Step [1/1], Loss: 2.4583\n",
            "Epoch [89/750], Step [1/1], Loss: 2.4564\n",
            "Epoch [90/750], Step [1/1], Loss: 2.4547\n",
            "Epoch [91/750], Step [1/1], Loss: 2.4530\n",
            "Epoch [92/750], Step [1/1], Loss: 2.4513\n",
            "Epoch [93/750], Step [1/1], Loss: 2.4496\n",
            "Epoch [94/750], Step [1/1], Loss: 2.4477\n",
            "Epoch [95/750], Step [1/1], Loss: 2.4460\n",
            "Epoch [96/750], Step [1/1], Loss: 2.4444\n",
            "Epoch [97/750], Step [1/1], Loss: 2.4428\n",
            "Epoch [98/750], Step [1/1], Loss: 2.4415\n",
            "Epoch [99/750], Step [1/1], Loss: 2.4402\n",
            "Epoch [100/750], Step [1/1], Loss: 2.4387\n",
            "Epoch [101/750], Step [1/1], Loss: 2.4377\n",
            "Epoch [102/750], Step [1/1], Loss: 2.4358\n",
            "Epoch [103/750], Step [1/1], Loss: 2.4343\n",
            "Epoch [104/750], Step [1/1], Loss: 2.4328\n",
            "Epoch [105/750], Step [1/1], Loss: 2.4317\n",
            "Epoch [106/750], Step [1/1], Loss: 2.4303\n",
            "Epoch [107/750], Step [1/1], Loss: 2.4289\n",
            "Epoch [108/750], Step [1/1], Loss: 2.4276\n",
            "Epoch [109/750], Step [1/1], Loss: 2.4260\n",
            "Epoch [110/750], Step [1/1], Loss: 2.4247\n",
            "Epoch [111/750], Step [1/1], Loss: 2.4231\n",
            "Epoch [112/750], Step [1/1], Loss: 2.4217\n",
            "Epoch [113/750], Step [1/1], Loss: 2.4209\n",
            "Epoch [114/750], Step [1/1], Loss: 2.4195\n",
            "Epoch [115/750], Step [1/1], Loss: 2.4185\n",
            "Epoch [116/750], Step [1/1], Loss: 2.4169\n",
            "Epoch [117/750], Step [1/1], Loss: 2.4154\n",
            "Epoch [118/750], Step [1/1], Loss: 2.4139\n",
            "Epoch [119/750], Step [1/1], Loss: 2.4130\n",
            "Epoch [120/750], Step [1/1], Loss: 2.4120\n",
            "Epoch [121/750], Step [1/1], Loss: 2.4102\n",
            "Epoch [122/750], Step [1/1], Loss: 2.4089\n",
            "Epoch [123/750], Step [1/1], Loss: 2.4077\n",
            "Epoch [124/750], Step [1/1], Loss: 2.4069\n",
            "Epoch [125/750], Step [1/1], Loss: 2.4055\n",
            "Epoch [126/750], Step [1/1], Loss: 2.4044\n",
            "Epoch [127/750], Step [1/1], Loss: 2.4033\n",
            "Epoch [128/750], Step [1/1], Loss: 2.4020\n",
            "Epoch [129/750], Step [1/1], Loss: 2.4006\n",
            "Epoch [130/750], Step [1/1], Loss: 2.3996\n",
            "Epoch [131/750], Step [1/1], Loss: 2.3987\n",
            "Epoch [132/750], Step [1/1], Loss: 2.3973\n",
            "Epoch [133/750], Step [1/1], Loss: 2.3963\n",
            "Epoch [134/750], Step [1/1], Loss: 2.3957\n",
            "Epoch [135/750], Step [1/1], Loss: 2.3940\n",
            "Epoch [136/750], Step [1/1], Loss: 2.3930\n",
            "Epoch [137/750], Step [1/1], Loss: 2.3925\n",
            "Epoch [138/750], Step [1/1], Loss: 2.3908\n",
            "Epoch [139/750], Step [1/1], Loss: 2.3898\n",
            "Epoch [140/750], Step [1/1], Loss: 2.3887\n",
            "Epoch [141/750], Step [1/1], Loss: 2.3871\n",
            "Epoch [142/750], Step [1/1], Loss: 2.3864\n",
            "Epoch [143/750], Step [1/1], Loss: 2.3859\n",
            "Epoch [144/750], Step [1/1], Loss: 2.3844\n",
            "Epoch [145/750], Step [1/1], Loss: 2.3833\n",
            "Epoch [146/750], Step [1/1], Loss: 2.3829\n",
            "Epoch [147/750], Step [1/1], Loss: 2.3815\n",
            "Epoch [148/750], Step [1/1], Loss: 2.3803\n",
            "Epoch [149/750], Step [1/1], Loss: 2.3799\n",
            "Epoch [150/750], Step [1/1], Loss: 2.3796\n",
            "Epoch [151/750], Step [1/1], Loss: 2.3778\n",
            "Epoch [152/750], Step [1/1], Loss: 2.3777\n",
            "Epoch [153/750], Step [1/1], Loss: 2.3754\n",
            "Epoch [154/750], Step [1/1], Loss: 2.3746\n",
            "Epoch [155/750], Step [1/1], Loss: 2.3749\n",
            "Epoch [156/750], Step [1/1], Loss: 2.3728\n",
            "Epoch [157/750], Step [1/1], Loss: 2.3726\n",
            "Epoch [158/750], Step [1/1], Loss: 2.3719\n",
            "Epoch [159/750], Step [1/1], Loss: 2.3704\n",
            "Epoch [160/750], Step [1/1], Loss: 2.3698\n",
            "Epoch [161/750], Step [1/1], Loss: 2.3688\n",
            "Epoch [162/750], Step [1/1], Loss: 2.3686\n",
            "Epoch [163/750], Step [1/1], Loss: 2.3669\n",
            "Epoch [164/750], Step [1/1], Loss: 2.3662\n",
            "Epoch [165/750], Step [1/1], Loss: 2.3658\n",
            "Epoch [166/750], Step [1/1], Loss: 2.3647\n",
            "Epoch [167/750], Step [1/1], Loss: 2.3636\n",
            "Epoch [168/750], Step [1/1], Loss: 2.3632\n",
            "Epoch [169/750], Step [1/1], Loss: 2.3624\n",
            "Epoch [170/750], Step [1/1], Loss: 2.3614\n",
            "Epoch [171/750], Step [1/1], Loss: 2.3611\n",
            "Epoch [172/750], Step [1/1], Loss: 2.3600\n",
            "Epoch [173/750], Step [1/1], Loss: 2.3598\n",
            "Epoch [174/750], Step [1/1], Loss: 2.3585\n",
            "Epoch [175/750], Step [1/1], Loss: 2.3582\n",
            "Epoch [176/750], Step [1/1], Loss: 2.3574\n",
            "Epoch [177/750], Step [1/1], Loss: 2.3561\n",
            "Epoch [178/750], Step [1/1], Loss: 2.3555\n",
            "Epoch [179/750], Step [1/1], Loss: 2.3546\n",
            "Epoch [180/750], Step [1/1], Loss: 2.3548\n",
            "Epoch [181/750], Step [1/1], Loss: 2.3533\n",
            "Epoch [182/750], Step [1/1], Loss: 2.3533\n",
            "Epoch [183/750], Step [1/1], Loss: 2.3522\n",
            "Epoch [184/750], Step [1/1], Loss: 2.3519\n",
            "Epoch [185/750], Step [1/1], Loss: 2.3500\n",
            "Epoch [186/750], Step [1/1], Loss: 2.3494\n",
            "Epoch [187/750], Step [1/1], Loss: 2.3491\n",
            "Epoch [188/750], Step [1/1], Loss: 2.3484\n",
            "Epoch [189/750], Step [1/1], Loss: 2.3477\n",
            "Epoch [190/750], Step [1/1], Loss: 2.3464\n",
            "Epoch [191/750], Step [1/1], Loss: 2.3468\n",
            "Epoch [192/750], Step [1/1], Loss: 2.3456\n",
            "Epoch [193/750], Step [1/1], Loss: 2.3455\n",
            "Epoch [194/750], Step [1/1], Loss: 2.3440\n",
            "Epoch [195/750], Step [1/1], Loss: 2.3442\n",
            "Epoch [196/750], Step [1/1], Loss: 2.3430\n",
            "Epoch [197/750], Step [1/1], Loss: 2.3427\n",
            "Epoch [198/750], Step [1/1], Loss: 2.3412\n",
            "Epoch [199/750], Step [1/1], Loss: 2.3414\n",
            "Epoch [200/750], Step [1/1], Loss: 2.3400\n",
            "Epoch [201/750], Step [1/1], Loss: 2.3403\n",
            "Epoch [202/750], Step [1/1], Loss: 2.3386\n",
            "Epoch [203/750], Step [1/1], Loss: 2.3387\n",
            "Epoch [204/750], Step [1/1], Loss: 2.3377\n",
            "Epoch [205/750], Step [1/1], Loss: 2.3370\n",
            "Epoch [206/750], Step [1/1], Loss: 2.3361\n",
            "Epoch [207/750], Step [1/1], Loss: 2.3355\n",
            "Epoch [208/750], Step [1/1], Loss: 2.3352\n",
            "Epoch [209/750], Step [1/1], Loss: 2.3345\n",
            "Epoch [210/750], Step [1/1], Loss: 2.3338\n",
            "Epoch [211/750], Step [1/1], Loss: 2.3330\n",
            "Epoch [212/750], Step [1/1], Loss: 2.3324\n",
            "Epoch [213/750], Step [1/1], Loss: 2.3319\n",
            "Epoch [214/750], Step [1/1], Loss: 2.3311\n",
            "Epoch [215/750], Step [1/1], Loss: 2.3309\n",
            "Epoch [216/750], Step [1/1], Loss: 2.3303\n",
            "Epoch [217/750], Step [1/1], Loss: 2.3307\n",
            "Epoch [218/750], Step [1/1], Loss: 2.3286\n",
            "Epoch [219/750], Step [1/1], Loss: 2.3293\n",
            "Epoch [220/750], Step [1/1], Loss: 2.3282\n",
            "Epoch [221/750], Step [1/1], Loss: 2.3270\n",
            "Epoch [222/750], Step [1/1], Loss: 2.3270\n",
            "Epoch [223/750], Step [1/1], Loss: 2.3254\n",
            "Epoch [224/750], Step [1/1], Loss: 2.3256\n",
            "Epoch [225/750], Step [1/1], Loss: 2.3249\n",
            "Epoch [226/750], Step [1/1], Loss: 2.3253\n",
            "Epoch [227/750], Step [1/1], Loss: 2.3234\n",
            "Epoch [228/750], Step [1/1], Loss: 2.3238\n",
            "Epoch [229/750], Step [1/1], Loss: 2.3220\n",
            "Epoch [230/750], Step [1/1], Loss: 2.3221\n",
            "Epoch [231/750], Step [1/1], Loss: 2.3210\n",
            "Epoch [232/750], Step [1/1], Loss: 2.3204\n",
            "Epoch [233/750], Step [1/1], Loss: 2.3207\n",
            "Epoch [234/750], Step [1/1], Loss: 2.3198\n",
            "Epoch [235/750], Step [1/1], Loss: 2.3200\n",
            "Epoch [236/750], Step [1/1], Loss: 2.3185\n",
            "Epoch [237/750], Step [1/1], Loss: 2.3176\n",
            "Epoch [238/750], Step [1/1], Loss: 2.3178\n",
            "Epoch [239/750], Step [1/1], Loss: 2.3169\n",
            "Epoch [240/750], Step [1/1], Loss: 2.3182\n",
            "Epoch [241/750], Step [1/1], Loss: 2.3159\n",
            "Epoch [242/750], Step [1/1], Loss: 2.3159\n",
            "Epoch [243/750], Step [1/1], Loss: 2.3152\n",
            "Epoch [244/750], Step [1/1], Loss: 2.3154\n",
            "Epoch [245/750], Step [1/1], Loss: 2.3156\n",
            "Epoch [246/750], Step [1/1], Loss: 2.3131\n",
            "Epoch [247/750], Step [1/1], Loss: 2.3126\n",
            "Epoch [248/750], Step [1/1], Loss: 2.3150\n",
            "Epoch [249/750], Step [1/1], Loss: 2.3126\n",
            "Epoch [250/750], Step [1/1], Loss: 2.3115\n",
            "Epoch [251/750], Step [1/1], Loss: 2.3148\n",
            "Epoch [252/750], Step [1/1], Loss: 2.3124\n",
            "Epoch [253/750], Step [1/1], Loss: 2.3100\n",
            "Epoch [254/750], Step [1/1], Loss: 2.3170\n",
            "Epoch [255/750], Step [1/1], Loss: 2.3109\n",
            "Epoch [256/750], Step [1/1], Loss: 2.3111\n",
            "Epoch [257/750], Step [1/1], Loss: 2.3140\n",
            "Epoch [258/750], Step [1/1], Loss: 2.3091\n",
            "Epoch [259/750], Step [1/1], Loss: 2.3088\n",
            "Epoch [260/750], Step [1/1], Loss: 2.3073\n",
            "Epoch [261/750], Step [1/1], Loss: 2.3099\n",
            "Epoch [262/750], Step [1/1], Loss: 2.3055\n",
            "Epoch [263/750], Step [1/1], Loss: 2.3061\n",
            "Epoch [264/750], Step [1/1], Loss: 2.3057\n",
            "Epoch [265/750], Step [1/1], Loss: 2.3046\n",
            "Epoch [266/750], Step [1/1], Loss: 2.3036\n",
            "Epoch [267/750], Step [1/1], Loss: 2.3027\n",
            "Epoch [268/750], Step [1/1], Loss: 2.3045\n",
            "Epoch [269/750], Step [1/1], Loss: 2.3021\n",
            "Epoch [270/750], Step [1/1], Loss: 2.3017\n",
            "Epoch [271/750], Step [1/1], Loss: 2.3009\n",
            "Epoch [272/750], Step [1/1], Loss: 2.3014\n",
            "Epoch [273/750], Step [1/1], Loss: 2.3001\n",
            "Epoch [274/750], Step [1/1], Loss: 2.2989\n",
            "Epoch [275/750], Step [1/1], Loss: 2.3001\n",
            "Epoch [276/750], Step [1/1], Loss: 2.2981\n",
            "Epoch [277/750], Step [1/1], Loss: 2.2980\n",
            "Epoch [278/750], Step [1/1], Loss: 2.2976\n",
            "Epoch [279/750], Step [1/1], Loss: 2.2975\n",
            "Epoch [280/750], Step [1/1], Loss: 2.2966\n",
            "Epoch [281/750], Step [1/1], Loss: 2.2955\n",
            "Epoch [282/750], Step [1/1], Loss: 2.2964\n",
            "Epoch [283/750], Step [1/1], Loss: 2.2949\n",
            "Epoch [284/750], Step [1/1], Loss: 2.2941\n",
            "Epoch [285/750], Step [1/1], Loss: 2.2941\n",
            "Epoch [286/750], Step [1/1], Loss: 2.2939\n",
            "Epoch [287/750], Step [1/1], Loss: 2.2924\n",
            "Epoch [288/750], Step [1/1], Loss: 2.2920\n",
            "Epoch [289/750], Step [1/1], Loss: 2.2928\n",
            "Epoch [290/750], Step [1/1], Loss: 2.2910\n",
            "Epoch [291/750], Step [1/1], Loss: 2.2908\n",
            "Epoch [292/750], Step [1/1], Loss: 2.2915\n",
            "Epoch [293/750], Step [1/1], Loss: 2.2897\n",
            "Epoch [294/750], Step [1/1], Loss: 2.2902\n",
            "Epoch [295/750], Step [1/1], Loss: 2.2893\n",
            "Epoch [296/750], Step [1/1], Loss: 2.2890\n",
            "Epoch [297/750], Step [1/1], Loss: 2.2886\n",
            "Epoch [298/750], Step [1/1], Loss: 2.2880\n",
            "Epoch [299/750], Step [1/1], Loss: 2.2882\n",
            "Epoch [300/750], Step [1/1], Loss: 2.2868\n",
            "Epoch [301/750], Step [1/1], Loss: 2.2864\n",
            "Epoch [302/750], Step [1/1], Loss: 2.2868\n",
            "Epoch [303/750], Step [1/1], Loss: 2.2860\n",
            "Epoch [304/750], Step [1/1], Loss: 2.2847\n",
            "Epoch [305/750], Step [1/1], Loss: 2.2856\n",
            "Epoch [306/750], Step [1/1], Loss: 2.2849\n",
            "Epoch [307/750], Step [1/1], Loss: 2.2840\n",
            "Epoch [308/750], Step [1/1], Loss: 2.2843\n",
            "Epoch [309/750], Step [1/1], Loss: 2.2838\n",
            "Epoch [310/750], Step [1/1], Loss: 2.2831\n",
            "Epoch [311/750], Step [1/1], Loss: 2.2828\n",
            "Epoch [312/750], Step [1/1], Loss: 2.2828\n",
            "Epoch [313/750], Step [1/1], Loss: 2.2818\n",
            "Epoch [314/750], Step [1/1], Loss: 2.2823\n",
            "Epoch [315/750], Step [1/1], Loss: 2.2811\n",
            "Epoch [316/750], Step [1/1], Loss: 2.2802\n",
            "Epoch [317/750], Step [1/1], Loss: 2.2810\n",
            "Epoch [318/750], Step [1/1], Loss: 2.2799\n",
            "Epoch [319/750], Step [1/1], Loss: 2.2788\n",
            "Epoch [320/750], Step [1/1], Loss: 2.2810\n",
            "Epoch [321/750], Step [1/1], Loss: 2.2795\n",
            "Epoch [322/750], Step [1/1], Loss: 2.2784\n",
            "Epoch [323/750], Step [1/1], Loss: 2.2809\n",
            "Epoch [324/750], Step [1/1], Loss: 2.2774\n",
            "Epoch [325/750], Step [1/1], Loss: 2.2780\n",
            "Epoch [326/750], Step [1/1], Loss: 2.2778\n",
            "Epoch [327/750], Step [1/1], Loss: 2.2766\n",
            "Epoch [328/750], Step [1/1], Loss: 2.2767\n",
            "Epoch [329/750], Step [1/1], Loss: 2.2756\n",
            "Epoch [330/750], Step [1/1], Loss: 2.2773\n",
            "Epoch [331/750], Step [1/1], Loss: 2.2752\n",
            "Epoch [332/750], Step [1/1], Loss: 2.2745\n",
            "Epoch [333/750], Step [1/1], Loss: 2.2753\n",
            "Epoch [334/750], Step [1/1], Loss: 2.2737\n",
            "Epoch [335/750], Step [1/1], Loss: 2.2743\n",
            "Epoch [336/750], Step [1/1], Loss: 2.2730\n",
            "Epoch [337/750], Step [1/1], Loss: 2.2738\n",
            "Epoch [338/750], Step [1/1], Loss: 2.2719\n",
            "Epoch [339/750], Step [1/1], Loss: 2.2716\n",
            "Epoch [340/750], Step [1/1], Loss: 2.2722\n",
            "Epoch [341/750], Step [1/1], Loss: 2.2709\n",
            "Epoch [342/750], Step [1/1], Loss: 2.2702\n",
            "Epoch [343/750], Step [1/1], Loss: 2.2709\n",
            "Epoch [344/750], Step [1/1], Loss: 2.2699\n",
            "Epoch [345/750], Step [1/1], Loss: 2.2689\n",
            "Epoch [346/750], Step [1/1], Loss: 2.2694\n",
            "Epoch [347/750], Step [1/1], Loss: 2.2684\n",
            "Epoch [348/750], Step [1/1], Loss: 2.2678\n",
            "Epoch [349/750], Step [1/1], Loss: 2.2685\n",
            "Epoch [350/750], Step [1/1], Loss: 2.2680\n",
            "Epoch [351/750], Step [1/1], Loss: 2.2667\n",
            "Epoch [352/750], Step [1/1], Loss: 2.2665\n",
            "Epoch [353/750], Step [1/1], Loss: 2.2670\n",
            "Epoch [354/750], Step [1/1], Loss: 2.2655\n",
            "Epoch [355/750], Step [1/1], Loss: 2.2652\n",
            "Epoch [356/750], Step [1/1], Loss: 2.2650\n",
            "Epoch [357/750], Step [1/1], Loss: 2.2645\n",
            "Epoch [358/750], Step [1/1], Loss: 2.2640\n",
            "Epoch [359/750], Step [1/1], Loss: 2.2635\n",
            "Epoch [360/750], Step [1/1], Loss: 2.2630\n",
            "Epoch [361/750], Step [1/1], Loss: 2.2628\n",
            "Epoch [362/750], Step [1/1], Loss: 2.2629\n",
            "Epoch [363/750], Step [1/1], Loss: 2.2622\n",
            "Epoch [364/750], Step [1/1], Loss: 2.2621\n",
            "Epoch [365/750], Step [1/1], Loss: 2.2619\n",
            "Epoch [366/750], Step [1/1], Loss: 2.2613\n",
            "Epoch [367/750], Step [1/1], Loss: 2.2605\n",
            "Epoch [368/750], Step [1/1], Loss: 2.2607\n",
            "Epoch [369/750], Step [1/1], Loss: 2.2599\n",
            "Epoch [370/750], Step [1/1], Loss: 2.2596\n",
            "Epoch [371/750], Step [1/1], Loss: 2.2597\n",
            "Epoch [372/750], Step [1/1], Loss: 2.2592\n",
            "Epoch [373/750], Step [1/1], Loss: 2.2588\n",
            "Epoch [374/750], Step [1/1], Loss: 2.2587\n",
            "Epoch [375/750], Step [1/1], Loss: 2.2578\n",
            "Epoch [376/750], Step [1/1], Loss: 2.2579\n",
            "Epoch [377/750], Step [1/1], Loss: 2.2572\n",
            "Epoch [378/750], Step [1/1], Loss: 2.2569\n",
            "Epoch [379/750], Step [1/1], Loss: 2.2567\n",
            "Epoch [380/750], Step [1/1], Loss: 2.2563\n",
            "Epoch [381/750], Step [1/1], Loss: 2.2561\n",
            "Epoch [382/750], Step [1/1], Loss: 2.2557\n",
            "Epoch [383/750], Step [1/1], Loss: 2.2552\n",
            "Epoch [384/750], Step [1/1], Loss: 2.2549\n",
            "Epoch [385/750], Step [1/1], Loss: 2.2554\n",
            "Epoch [386/750], Step [1/1], Loss: 2.2552\n",
            "Epoch [387/750], Step [1/1], Loss: 2.2550\n",
            "Epoch [388/750], Step [1/1], Loss: 2.2541\n",
            "Epoch [389/750], Step [1/1], Loss: 2.2539\n",
            "Epoch [390/750], Step [1/1], Loss: 2.2536\n",
            "Epoch [391/750], Step [1/1], Loss: 2.2531\n",
            "Epoch [392/750], Step [1/1], Loss: 2.2532\n",
            "Epoch [393/750], Step [1/1], Loss: 2.2527\n",
            "Epoch [394/750], Step [1/1], Loss: 2.2528\n",
            "Epoch [395/750], Step [1/1], Loss: 2.2521\n",
            "Epoch [396/750], Step [1/1], Loss: 2.2523\n",
            "Epoch [397/750], Step [1/1], Loss: 2.2511\n",
            "Epoch [398/750], Step [1/1], Loss: 2.2517\n",
            "Epoch [399/750], Step [1/1], Loss: 2.2510\n",
            "Epoch [400/750], Step [1/1], Loss: 2.2510\n",
            "Epoch [401/750], Step [1/1], Loss: 2.2508\n",
            "Epoch [402/750], Step [1/1], Loss: 2.2513\n",
            "Epoch [403/750], Step [1/1], Loss: 2.2494\n",
            "Epoch [404/750], Step [1/1], Loss: 2.2515\n",
            "Epoch [405/750], Step [1/1], Loss: 2.2513\n",
            "Epoch [406/750], Step [1/1], Loss: 2.2496\n",
            "Epoch [407/750], Step [1/1], Loss: 2.2531\n",
            "Epoch [408/750], Step [1/1], Loss: 2.2523\n",
            "Epoch [409/750], Step [1/1], Loss: 2.2518\n",
            "Epoch [410/750], Step [1/1], Loss: 2.2511\n",
            "Epoch [411/750], Step [1/1], Loss: 2.2489\n",
            "Epoch [412/750], Step [1/1], Loss: 2.2503\n",
            "Epoch [413/750], Step [1/1], Loss: 2.2481\n",
            "Epoch [414/750], Step [1/1], Loss: 2.2502\n",
            "Epoch [415/750], Step [1/1], Loss: 2.2502\n",
            "Epoch [416/750], Step [1/1], Loss: 2.2495\n",
            "Epoch [417/750], Step [1/1], Loss: 2.2501\n",
            "Epoch [418/750], Step [1/1], Loss: 2.2505\n",
            "Epoch [419/750], Step [1/1], Loss: 2.2491\n",
            "Epoch [420/750], Step [1/1], Loss: 2.2483\n",
            "Epoch [421/750], Step [1/1], Loss: 2.2488\n",
            "Epoch [422/750], Step [1/1], Loss: 2.2476\n",
            "Epoch [423/750], Step [1/1], Loss: 2.2474\n",
            "Epoch [424/750], Step [1/1], Loss: 2.2470\n",
            "Epoch [425/750], Step [1/1], Loss: 2.2457\n",
            "Epoch [426/750], Step [1/1], Loss: 2.2472\n",
            "Epoch [427/750], Step [1/1], Loss: 2.2452\n",
            "Epoch [428/750], Step [1/1], Loss: 2.2461\n",
            "Epoch [429/750], Step [1/1], Loss: 2.2456\n",
            "Epoch [430/750], Step [1/1], Loss: 2.2446\n",
            "Epoch [431/750], Step [1/1], Loss: 2.2458\n",
            "Epoch [432/750], Step [1/1], Loss: 2.2444\n",
            "Epoch [433/750], Step [1/1], Loss: 2.2439\n",
            "Epoch [434/750], Step [1/1], Loss: 2.2442\n",
            "Epoch [435/750], Step [1/1], Loss: 2.2444\n",
            "Epoch [436/750], Step [1/1], Loss: 2.2432\n",
            "Epoch [437/750], Step [1/1], Loss: 2.2443\n",
            "Epoch [438/750], Step [1/1], Loss: 2.2432\n",
            "Epoch [439/750], Step [1/1], Loss: 2.2414\n",
            "Epoch [440/750], Step [1/1], Loss: 2.2431\n",
            "Epoch [441/750], Step [1/1], Loss: 2.2421\n",
            "Epoch [442/750], Step [1/1], Loss: 2.2410\n",
            "Epoch [443/750], Step [1/1], Loss: 2.2424\n",
            "Epoch [444/750], Step [1/1], Loss: 2.2413\n",
            "Epoch [445/750], Step [1/1], Loss: 2.2402\n",
            "Epoch [446/750], Step [1/1], Loss: 2.2415\n",
            "Epoch [447/750], Step [1/1], Loss: 2.2401\n",
            "Epoch [448/750], Step [1/1], Loss: 2.2397\n",
            "Epoch [449/750], Step [1/1], Loss: 2.2399\n",
            "Epoch [450/750], Step [1/1], Loss: 2.2391\n",
            "Epoch [451/750], Step [1/1], Loss: 2.2384\n",
            "Epoch [452/750], Step [1/1], Loss: 2.2383\n",
            "Epoch [453/750], Step [1/1], Loss: 2.2382\n",
            "Epoch [454/750], Step [1/1], Loss: 2.2372\n",
            "Epoch [455/750], Step [1/1], Loss: 2.2376\n",
            "Epoch [456/750], Step [1/1], Loss: 2.2367\n",
            "Epoch [457/750], Step [1/1], Loss: 2.2364\n",
            "Epoch [458/750], Step [1/1], Loss: 2.2368\n",
            "Epoch [459/750], Step [1/1], Loss: 2.2358\n",
            "Epoch [460/750], Step [1/1], Loss: 2.2359\n",
            "Epoch [461/750], Step [1/1], Loss: 2.2358\n",
            "Epoch [462/750], Step [1/1], Loss: 2.2352\n",
            "Epoch [463/750], Step [1/1], Loss: 2.2348\n",
            "Epoch [464/750], Step [1/1], Loss: 2.2353\n",
            "Epoch [465/750], Step [1/1], Loss: 2.2346\n",
            "Epoch [466/750], Step [1/1], Loss: 2.2344\n",
            "Epoch [467/750], Step [1/1], Loss: 2.2352\n",
            "Epoch [468/750], Step [1/1], Loss: 2.2347\n",
            "Epoch [469/750], Step [1/1], Loss: 2.2340\n",
            "Epoch [470/750], Step [1/1], Loss: 2.2348\n",
            "Epoch [471/750], Step [1/1], Loss: 2.2340\n",
            "Epoch [472/750], Step [1/1], Loss: 2.2330\n",
            "Epoch [473/750], Step [1/1], Loss: 2.2338\n",
            "Epoch [474/750], Step [1/1], Loss: 2.2329\n",
            "Epoch [475/750], Step [1/1], Loss: 2.2322\n",
            "Epoch [476/750], Step [1/1], Loss: 2.2325\n",
            "Epoch [477/750], Step [1/1], Loss: 2.2322\n",
            "Epoch [478/750], Step [1/1], Loss: 2.2316\n",
            "Epoch [479/750], Step [1/1], Loss: 2.2316\n",
            "Epoch [480/750], Step [1/1], Loss: 2.2314\n",
            "Epoch [481/750], Step [1/1], Loss: 2.2306\n",
            "Epoch [482/750], Step [1/1], Loss: 2.2303\n",
            "Epoch [483/750], Step [1/1], Loss: 2.2305\n",
            "Epoch [484/750], Step [1/1], Loss: 2.2299\n",
            "Epoch [485/750], Step [1/1], Loss: 2.2298\n",
            "Epoch [486/750], Step [1/1], Loss: 2.2297\n",
            "Epoch [487/750], Step [1/1], Loss: 2.2294\n",
            "Epoch [488/750], Step [1/1], Loss: 2.2292\n",
            "Epoch [489/750], Step [1/1], Loss: 2.2290\n",
            "Epoch [490/750], Step [1/1], Loss: 2.2286\n",
            "Epoch [491/750], Step [1/1], Loss: 2.2282\n",
            "Epoch [492/750], Step [1/1], Loss: 2.2279\n",
            "Epoch [493/750], Step [1/1], Loss: 2.2277\n",
            "Epoch [494/750], Step [1/1], Loss: 2.2275\n",
            "Epoch [495/750], Step [1/1], Loss: 2.2271\n",
            "Epoch [496/750], Step [1/1], Loss: 2.2274\n",
            "Epoch [497/750], Step [1/1], Loss: 2.2266\n",
            "Epoch [498/750], Step [1/1], Loss: 2.2264\n",
            "Epoch [499/750], Step [1/1], Loss: 2.2264\n",
            "Epoch [500/750], Step [1/1], Loss: 2.2264\n",
            "Epoch [501/750], Step [1/1], Loss: 2.2259\n",
            "Epoch [502/750], Step [1/1], Loss: 2.2260\n",
            "Epoch [503/750], Step [1/1], Loss: 2.2256\n",
            "Epoch [504/750], Step [1/1], Loss: 2.2253\n",
            "Epoch [505/750], Step [1/1], Loss: 2.2250\n",
            "Epoch [506/750], Step [1/1], Loss: 2.2246\n",
            "Epoch [507/750], Step [1/1], Loss: 2.2246\n",
            "Epoch [508/750], Step [1/1], Loss: 2.2243\n",
            "Epoch [509/750], Step [1/1], Loss: 2.2241\n",
            "Epoch [510/750], Step [1/1], Loss: 2.2238\n",
            "Epoch [511/750], Step [1/1], Loss: 2.2237\n",
            "Epoch [512/750], Step [1/1], Loss: 2.2237\n",
            "Epoch [513/750], Step [1/1], Loss: 2.2234\n",
            "Epoch [514/750], Step [1/1], Loss: 2.2232\n",
            "Epoch [515/750], Step [1/1], Loss: 2.2229\n",
            "Epoch [516/750], Step [1/1], Loss: 2.2233\n",
            "Epoch [517/750], Step [1/1], Loss: 2.2224\n",
            "Epoch [518/750], Step [1/1], Loss: 2.2225\n",
            "Epoch [519/750], Step [1/1], Loss: 2.2224\n",
            "Epoch [520/750], Step [1/1], Loss: 2.2224\n",
            "Epoch [521/750], Step [1/1], Loss: 2.2217\n",
            "Epoch [522/750], Step [1/1], Loss: 2.2224\n",
            "Epoch [523/750], Step [1/1], Loss: 2.2217\n",
            "Epoch [524/750], Step [1/1], Loss: 2.2217\n",
            "Epoch [525/750], Step [1/1], Loss: 2.2232\n",
            "Epoch [526/750], Step [1/1], Loss: 2.2219\n",
            "Epoch [527/750], Step [1/1], Loss: 2.2231\n",
            "Epoch [528/750], Step [1/1], Loss: 2.2246\n",
            "Epoch [529/750], Step [1/1], Loss: 2.2228\n",
            "Epoch [530/750], Step [1/1], Loss: 2.2248\n",
            "Epoch [531/750], Step [1/1], Loss: 2.2246\n",
            "Epoch [532/750], Step [1/1], Loss: 2.2226\n",
            "Epoch [533/750], Step [1/1], Loss: 2.2238\n",
            "Epoch [534/750], Step [1/1], Loss: 2.2217\n",
            "Epoch [535/750], Step [1/1], Loss: 2.2215\n",
            "Epoch [536/750], Step [1/1], Loss: 2.2219\n",
            "Epoch [537/750], Step [1/1], Loss: 2.2216\n",
            "Epoch [538/750], Step [1/1], Loss: 2.2214\n",
            "Epoch [539/750], Step [1/1], Loss: 2.2222\n",
            "Epoch [540/750], Step [1/1], Loss: 2.2223\n",
            "Epoch [541/750], Step [1/1], Loss: 2.2215\n",
            "Epoch [542/750], Step [1/1], Loss: 2.2208\n",
            "Epoch [543/750], Step [1/1], Loss: 2.2196\n",
            "Epoch [544/750], Step [1/1], Loss: 2.2198\n",
            "Epoch [545/750], Step [1/1], Loss: 2.2191\n",
            "Epoch [546/750], Step [1/1], Loss: 2.2200\n",
            "Epoch [547/750], Step [1/1], Loss: 2.2194\n",
            "Epoch [548/750], Step [1/1], Loss: 2.2204\n",
            "Epoch [549/750], Step [1/1], Loss: 2.2191\n",
            "Epoch [550/750], Step [1/1], Loss: 2.2194\n",
            "Epoch [551/750], Step [1/1], Loss: 2.2184\n",
            "Epoch [552/750], Step [1/1], Loss: 2.2182\n",
            "Epoch [553/750], Step [1/1], Loss: 2.2183\n",
            "Epoch [554/750], Step [1/1], Loss: 2.2168\n",
            "Epoch [555/750], Step [1/1], Loss: 2.2180\n",
            "Epoch [556/750], Step [1/1], Loss: 2.2172\n",
            "Epoch [557/750], Step [1/1], Loss: 2.2169\n",
            "Epoch [558/750], Step [1/1], Loss: 2.2167\n",
            "Epoch [559/750], Step [1/1], Loss: 2.2163\n",
            "Epoch [560/750], Step [1/1], Loss: 2.2159\n",
            "Epoch [561/750], Step [1/1], Loss: 2.2159\n",
            "Epoch [562/750], Step [1/1], Loss: 2.2164\n",
            "Epoch [563/750], Step [1/1], Loss: 2.2164\n",
            "Epoch [564/750], Step [1/1], Loss: 2.2156\n",
            "Epoch [565/750], Step [1/1], Loss: 2.2150\n",
            "Epoch [566/750], Step [1/1], Loss: 2.2150\n",
            "Epoch [567/750], Step [1/1], Loss: 2.2147\n",
            "Epoch [568/750], Step [1/1], Loss: 2.2146\n",
            "Epoch [569/750], Step [1/1], Loss: 2.2142\n",
            "Epoch [570/750], Step [1/1], Loss: 2.2143\n",
            "Epoch [571/750], Step [1/1], Loss: 2.2138\n",
            "Epoch [572/750], Step [1/1], Loss: 2.2134\n",
            "Epoch [573/750], Step [1/1], Loss: 2.2133\n",
            "Epoch [574/750], Step [1/1], Loss: 2.2130\n",
            "Epoch [575/750], Step [1/1], Loss: 2.2129\n",
            "Epoch [576/750], Step [1/1], Loss: 2.2130\n",
            "Epoch [577/750], Step [1/1], Loss: 2.2129\n",
            "Epoch [578/750], Step [1/1], Loss: 2.2125\n",
            "Epoch [579/750], Step [1/1], Loss: 2.2129\n",
            "Epoch [580/750], Step [1/1], Loss: 2.2127\n",
            "Epoch [581/750], Step [1/1], Loss: 2.2123\n",
            "Epoch [582/750], Step [1/1], Loss: 2.2127\n",
            "Epoch [583/750], Step [1/1], Loss: 2.2127\n",
            "Epoch [584/750], Step [1/1], Loss: 2.2119\n",
            "Epoch [585/750], Step [1/1], Loss: 2.2123\n",
            "Epoch [586/750], Step [1/1], Loss: 2.2128\n",
            "Epoch [587/750], Step [1/1], Loss: 2.2118\n",
            "Epoch [588/750], Step [1/1], Loss: 2.2124\n",
            "Epoch [589/750], Step [1/1], Loss: 2.2126\n",
            "Epoch [590/750], Step [1/1], Loss: 2.2113\n",
            "Epoch [591/750], Step [1/1], Loss: 2.2122\n",
            "Epoch [592/750], Step [1/1], Loss: 2.2122\n",
            "Epoch [593/750], Step [1/1], Loss: 2.2117\n",
            "Epoch [594/750], Step [1/1], Loss: 2.2107\n",
            "Epoch [595/750], Step [1/1], Loss: 2.2103\n",
            "Epoch [596/750], Step [1/1], Loss: 2.2108\n",
            "Epoch [597/750], Step [1/1], Loss: 2.2103\n",
            "Epoch [598/750], Step [1/1], Loss: 2.2104\n",
            "Epoch [599/750], Step [1/1], Loss: 2.2104\n",
            "Epoch [600/750], Step [1/1], Loss: 2.2100\n",
            "Epoch [601/750], Step [1/1], Loss: 2.2106\n",
            "Epoch [602/750], Step [1/1], Loss: 2.2095\n",
            "Epoch [603/750], Step [1/1], Loss: 2.2091\n",
            "Epoch [604/750], Step [1/1], Loss: 2.2094\n",
            "Epoch [605/750], Step [1/1], Loss: 2.2085\n",
            "Epoch [606/750], Step [1/1], Loss: 2.2084\n",
            "Epoch [607/750], Step [1/1], Loss: 2.2083\n",
            "Epoch [608/750], Step [1/1], Loss: 2.2077\n",
            "Epoch [609/750], Step [1/1], Loss: 2.2076\n",
            "Epoch [610/750], Step [1/1], Loss: 2.2076\n",
            "Epoch [611/750], Step [1/1], Loss: 2.2070\n",
            "Epoch [612/750], Step [1/1], Loss: 2.2070\n",
            "Epoch [613/750], Step [1/1], Loss: 2.2068\n",
            "Epoch [614/750], Step [1/1], Loss: 2.2064\n",
            "Epoch [615/750], Step [1/1], Loss: 2.2066\n",
            "Epoch [616/750], Step [1/1], Loss: 2.2060\n",
            "Epoch [617/750], Step [1/1], Loss: 2.2058\n",
            "Epoch [618/750], Step [1/1], Loss: 2.2061\n",
            "Epoch [619/750], Step [1/1], Loss: 2.2057\n",
            "Epoch [620/750], Step [1/1], Loss: 2.2056\n",
            "Epoch [621/750], Step [1/1], Loss: 2.2054\n",
            "Epoch [622/750], Step [1/1], Loss: 2.2051\n",
            "Epoch [623/750], Step [1/1], Loss: 2.2050\n",
            "Epoch [624/750], Step [1/1], Loss: 2.2049\n",
            "Epoch [625/750], Step [1/1], Loss: 2.2047\n",
            "Epoch [626/750], Step [1/1], Loss: 2.2044\n",
            "Epoch [627/750], Step [1/1], Loss: 2.2043\n",
            "Epoch [628/750], Step [1/1], Loss: 2.2041\n",
            "Epoch [629/750], Step [1/1], Loss: 2.2040\n",
            "Epoch [630/750], Step [1/1], Loss: 2.2039\n",
            "Epoch [631/750], Step [1/1], Loss: 2.2036\n",
            "Epoch [632/750], Step [1/1], Loss: 2.2034\n",
            "Epoch [633/750], Step [1/1], Loss: 2.2033\n",
            "Epoch [634/750], Step [1/1], Loss: 2.2030\n",
            "Epoch [635/750], Step [1/1], Loss: 2.2029\n",
            "Epoch [636/750], Step [1/1], Loss: 2.2028\n",
            "Epoch [637/750], Step [1/1], Loss: 2.2026\n",
            "Epoch [638/750], Step [1/1], Loss: 2.2026\n",
            "Epoch [639/750], Step [1/1], Loss: 2.2025\n",
            "Epoch [640/750], Step [1/1], Loss: 2.2024\n",
            "Epoch [641/750], Step [1/1], Loss: 2.2022\n",
            "Epoch [642/750], Step [1/1], Loss: 2.2020\n",
            "Epoch [643/750], Step [1/1], Loss: 2.2018\n",
            "Epoch [644/750], Step [1/1], Loss: 2.2017\n",
            "Epoch [645/750], Step [1/1], Loss: 2.2015\n",
            "Epoch [646/750], Step [1/1], Loss: 2.2014\n",
            "Epoch [647/750], Step [1/1], Loss: 2.2017\n",
            "Epoch [648/750], Step [1/1], Loss: 2.2013\n",
            "Epoch [649/750], Step [1/1], Loss: 2.2009\n",
            "Epoch [650/750], Step [1/1], Loss: 2.2011\n",
            "Epoch [651/750], Step [1/1], Loss: 2.2011\n",
            "Epoch [652/750], Step [1/1], Loss: 2.2009\n",
            "Epoch [653/750], Step [1/1], Loss: 2.2007\n",
            "Epoch [654/750], Step [1/1], Loss: 2.2003\n",
            "Epoch [655/750], Step [1/1], Loss: 2.2006\n",
            "Epoch [656/750], Step [1/1], Loss: 2.2006\n",
            "Epoch [657/750], Step [1/1], Loss: 2.2001\n",
            "Epoch [658/750], Step [1/1], Loss: 2.2005\n",
            "Epoch [659/750], Step [1/1], Loss: 2.2004\n",
            "Epoch [660/750], Step [1/1], Loss: 2.2006\n",
            "Epoch [661/750], Step [1/1], Loss: 2.2008\n",
            "Epoch [662/750], Step [1/1], Loss: 2.2009\n",
            "Epoch [663/750], Step [1/1], Loss: 2.2007\n",
            "Epoch [664/750], Step [1/1], Loss: 2.2016\n",
            "Epoch [665/750], Step [1/1], Loss: 2.2021\n",
            "Epoch [666/750], Step [1/1], Loss: 2.2019\n",
            "Epoch [667/750], Step [1/1], Loss: 2.2003\n",
            "Epoch [668/750], Step [1/1], Loss: 2.2004\n",
            "Epoch [669/750], Step [1/1], Loss: 2.2019\n",
            "Epoch [670/750], Step [1/1], Loss: 2.2007\n",
            "Epoch [671/750], Step [1/1], Loss: 2.2010\n",
            "Epoch [672/750], Step [1/1], Loss: 2.1997\n",
            "Epoch [673/750], Step [1/1], Loss: 2.2007\n",
            "Epoch [674/750], Step [1/1], Loss: 2.1994\n",
            "Epoch [675/750], Step [1/1], Loss: 2.2001\n",
            "Epoch [676/750], Step [1/1], Loss: 2.2008\n",
            "Epoch [677/750], Step [1/1], Loss: 2.1994\n",
            "Epoch [678/750], Step [1/1], Loss: 2.2030\n",
            "Epoch [679/750], Step [1/1], Loss: 2.2083\n",
            "Epoch [680/750], Step [1/1], Loss: 2.2049\n",
            "Epoch [681/750], Step [1/1], Loss: 2.2099\n",
            "Epoch [682/750], Step [1/1], Loss: 2.2058\n",
            "Epoch [683/750], Step [1/1], Loss: 2.2098\n",
            "Epoch [684/750], Step [1/1], Loss: 2.2103\n",
            "Epoch [685/750], Step [1/1], Loss: 2.2069\n",
            "Epoch [686/750], Step [1/1], Loss: 2.2080\n",
            "Epoch [687/750], Step [1/1], Loss: 2.2071\n",
            "Epoch [688/750], Step [1/1], Loss: 2.2166\n",
            "Epoch [689/750], Step [1/1], Loss: 2.2089\n",
            "Epoch [690/750], Step [1/1], Loss: 2.2168\n",
            "Epoch [691/750], Step [1/1], Loss: 2.2112\n",
            "Epoch [692/750], Step [1/1], Loss: 2.2195\n",
            "Epoch [693/750], Step [1/1], Loss: 2.2123\n",
            "Epoch [694/750], Step [1/1], Loss: 2.2098\n",
            "Epoch [695/750], Step [1/1], Loss: 2.2099\n",
            "Epoch [696/750], Step [1/1], Loss: 2.2074\n",
            "Epoch [697/750], Step [1/1], Loss: 2.2144\n",
            "Epoch [698/750], Step [1/1], Loss: 2.2086\n",
            "Epoch [699/750], Step [1/1], Loss: 2.2098\n",
            "Epoch [700/750], Step [1/1], Loss: 2.2085\n",
            "Epoch [701/750], Step [1/1], Loss: 2.2075\n",
            "Epoch [702/750], Step [1/1], Loss: 2.2084\n",
            "Epoch [703/750], Step [1/1], Loss: 2.2077\n",
            "Epoch [704/750], Step [1/1], Loss: 2.2047\n",
            "Epoch [705/750], Step [1/1], Loss: 2.2063\n",
            "Epoch [706/750], Step [1/1], Loss: 2.2041\n",
            "Epoch [707/750], Step [1/1], Loss: 2.2061\n",
            "Epoch [708/750], Step [1/1], Loss: 2.2050\n",
            "Epoch [709/750], Step [1/1], Loss: 2.2038\n",
            "Epoch [710/750], Step [1/1], Loss: 2.2028\n",
            "Epoch [711/750], Step [1/1], Loss: 2.2019\n",
            "Epoch [712/750], Step [1/1], Loss: 2.2035\n",
            "Epoch [713/750], Step [1/1], Loss: 2.2014\n",
            "Epoch [714/750], Step [1/1], Loss: 2.2022\n",
            "Epoch [715/750], Step [1/1], Loss: 2.2014\n",
            "Epoch [716/750], Step [1/1], Loss: 2.2001\n",
            "Epoch [717/750], Step [1/1], Loss: 2.2002\n",
            "Epoch [718/750], Step [1/1], Loss: 2.2003\n",
            "Epoch [719/750], Step [1/1], Loss: 2.1987\n",
            "Epoch [720/750], Step [1/1], Loss: 2.1991\n",
            "Epoch [721/750], Step [1/1], Loss: 2.1976\n",
            "Epoch [722/750], Step [1/1], Loss: 2.1976\n",
            "Epoch [723/750], Step [1/1], Loss: 2.1975\n",
            "Epoch [724/750], Step [1/1], Loss: 2.1973\n",
            "Epoch [725/750], Step [1/1], Loss: 2.1967\n",
            "Epoch [726/750], Step [1/1], Loss: 2.1969\n",
            "Epoch [727/750], Step [1/1], Loss: 2.1961\n",
            "Epoch [728/750], Step [1/1], Loss: 2.1960\n",
            "Epoch [729/750], Step [1/1], Loss: 2.1955\n",
            "Epoch [730/750], Step [1/1], Loss: 2.1954\n",
            "Epoch [731/750], Step [1/1], Loss: 2.1950\n",
            "Epoch [732/750], Step [1/1], Loss: 2.1946\n",
            "Epoch [733/750], Step [1/1], Loss: 2.1947\n",
            "Epoch [734/750], Step [1/1], Loss: 2.1944\n",
            "Epoch [735/750], Step [1/1], Loss: 2.1940\n",
            "Epoch [736/750], Step [1/1], Loss: 2.1939\n",
            "Epoch [737/750], Step [1/1], Loss: 2.1935\n",
            "Epoch [738/750], Step [1/1], Loss: 2.1934\n",
            "Epoch [739/750], Step [1/1], Loss: 2.1932\n",
            "Epoch [740/750], Step [1/1], Loss: 2.1931\n",
            "Epoch [741/750], Step [1/1], Loss: 2.1929\n",
            "Epoch [742/750], Step [1/1], Loss: 2.1928\n",
            "Epoch [743/750], Step [1/1], Loss: 2.1928\n",
            "Epoch [744/750], Step [1/1], Loss: 2.1925\n",
            "Epoch [745/750], Step [1/1], Loss: 2.1923\n",
            "Epoch [746/750], Step [1/1], Loss: 2.1920\n",
            "Epoch [747/750], Step [1/1], Loss: 2.1920\n",
            "Epoch [748/750], Step [1/1], Loss: 2.1917\n",
            "Epoch [749/750], Step [1/1], Loss: 2.1915\n",
            "Epoch [750/750], Step [1/1], Loss: 2.1916\n",
            "input:  torch.Size([20, 2264, 1, 700])  output:  torch.Size([20, 2264, 20])\n",
            "1507.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 66.56% (66.56%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ndn7NTlSLEkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "dt = 10_000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59apVTfzvHT-",
        "outputId": "ba593a68-d284-46a8-b9ca-d94ee295d9d2"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 4096, 1, 700]) torch.Size([4096]) tensor([ 3, 17,  0,  ...,  8, 10,  3])\n",
            "Epoch [1/100], Step [1/2], Loss: 3.0022\n",
            "Epoch [2/100], Step [1/2], Loss: 3.0007\n",
            "Epoch [3/100], Step [1/2], Loss: 2.9964\n",
            "Epoch [4/100], Step [1/2], Loss: 2.9917\n",
            "Epoch [5/100], Step [1/2], Loss: 2.9790\n",
            "Epoch [6/100], Step [1/2], Loss: 2.9673\n",
            "Epoch [7/100], Step [1/2], Loss: 2.9600\n",
            "Epoch [8/100], Step [1/2], Loss: 2.9505\n",
            "Epoch [9/100], Step [1/2], Loss: 2.9433\n",
            "Epoch [10/100], Step [1/2], Loss: 2.9335\n",
            "Epoch [11/100], Step [1/2], Loss: 2.9281\n",
            "Epoch [12/100], Step [1/2], Loss: 2.9217\n",
            "Epoch [13/100], Step [1/2], Loss: 2.9128\n",
            "Epoch [14/100], Step [1/2], Loss: 2.9073\n",
            "Epoch [15/100], Step [1/2], Loss: 2.9046\n",
            "Epoch [16/100], Step [1/2], Loss: 2.8967\n",
            "Epoch [17/100], Step [1/2], Loss: 2.8886\n",
            "Epoch [18/100], Step [1/2], Loss: 2.8832\n",
            "Epoch [19/100], Step [1/2], Loss: 2.8761\n",
            "Epoch [20/100], Step [1/2], Loss: 2.8693\n",
            "Epoch [21/100], Step [1/2], Loss: 2.8599\n",
            "Epoch [22/100], Step [1/2], Loss: 2.8539\n",
            "Epoch [23/100], Step [1/2], Loss: 2.8459\n",
            "Epoch [24/100], Step [1/2], Loss: 2.8380\n",
            "Epoch [25/100], Step [1/2], Loss: 2.8288\n",
            "Epoch [26/100], Step [1/2], Loss: 2.8231\n",
            "Epoch [27/100], Step [1/2], Loss: 2.8202\n",
            "Epoch [28/100], Step [1/2], Loss: 2.8139\n",
            "Epoch [29/100], Step [1/2], Loss: 2.8065\n",
            "Epoch [30/100], Step [1/2], Loss: 2.7999\n",
            "Epoch [31/100], Step [1/2], Loss: 2.7955\n",
            "Epoch [32/100], Step [1/2], Loss: 2.7915\n",
            "Epoch [33/100], Step [1/2], Loss: 2.7863\n",
            "Epoch [34/100], Step [1/2], Loss: 2.7810\n",
            "Epoch [35/100], Step [1/2], Loss: 2.7739\n",
            "Epoch [36/100], Step [1/2], Loss: 2.7713\n",
            "Epoch [37/100], Step [1/2], Loss: 2.7665\n",
            "Epoch [38/100], Step [1/2], Loss: 2.7593\n",
            "Epoch [39/100], Step [1/2], Loss: 2.7592\n",
            "Epoch [40/100], Step [1/2], Loss: 2.7515\n",
            "Epoch [41/100], Step [1/2], Loss: 2.7494\n",
            "Epoch [42/100], Step [1/2], Loss: 2.7477\n",
            "Epoch [43/100], Step [1/2], Loss: 2.7393\n",
            "Epoch [44/100], Step [1/2], Loss: 2.7370\n",
            "Epoch [45/100], Step [1/2], Loss: 2.7344\n",
            "Epoch [46/100], Step [1/2], Loss: 2.7289\n",
            "Epoch [47/100], Step [1/2], Loss: 2.7317\n",
            "Epoch [48/100], Step [1/2], Loss: 2.7208\n",
            "Epoch [49/100], Step [1/2], Loss: 2.7177\n",
            "Epoch [50/100], Step [1/2], Loss: 2.7150\n",
            "Epoch [51/100], Step [1/2], Loss: 2.7154\n",
            "Epoch [52/100], Step [1/2], Loss: 2.7122\n",
            "Epoch [53/100], Step [1/2], Loss: 2.7064\n",
            "Epoch [54/100], Step [1/2], Loss: 2.7082\n",
            "Epoch [55/100], Step [1/2], Loss: 2.7008\n",
            "Epoch [56/100], Step [1/2], Loss: 2.6981\n",
            "Epoch [57/100], Step [1/2], Loss: 2.6951\n",
            "Epoch [58/100], Step [1/2], Loss: 2.6918\n",
            "Epoch [59/100], Step [1/2], Loss: 2.6895\n",
            "Epoch [60/100], Step [1/2], Loss: 2.6850\n",
            "Epoch [61/100], Step [1/2], Loss: 2.6864\n",
            "Epoch [62/100], Step [1/2], Loss: 2.6851\n",
            "Epoch [63/100], Step [1/2], Loss: 2.6776\n",
            "Epoch [64/100], Step [1/2], Loss: 2.6765\n",
            "Epoch [65/100], Step [1/2], Loss: 2.6764\n",
            "Epoch [66/100], Step [1/2], Loss: 2.6699\n",
            "Epoch [67/100], Step [1/2], Loss: 2.6692\n",
            "Epoch [68/100], Step [1/2], Loss: 2.6696\n",
            "Epoch [69/100], Step [1/2], Loss: 2.6700\n",
            "Epoch [70/100], Step [1/2], Loss: 2.6665\n",
            "Epoch [71/100], Step [1/2], Loss: 2.6609\n",
            "Epoch [72/100], Step [1/2], Loss: 2.6617\n",
            "Epoch [73/100], Step [1/2], Loss: 2.6605\n",
            "Epoch [74/100], Step [1/2], Loss: 2.6548\n",
            "Epoch [75/100], Step [1/2], Loss: 2.6530\n",
            "Epoch [76/100], Step [1/2], Loss: 2.6528\n",
            "Epoch [77/100], Step [1/2], Loss: 2.6534\n",
            "Epoch [78/100], Step [1/2], Loss: 2.6491\n",
            "Epoch [79/100], Step [1/2], Loss: 2.6484\n",
            "Epoch [80/100], Step [1/2], Loss: 2.6495\n",
            "Epoch [81/100], Step [1/2], Loss: 2.6475\n",
            "Epoch [82/100], Step [1/2], Loss: 2.6460\n",
            "Epoch [83/100], Step [1/2], Loss: 2.6404\n",
            "Epoch [84/100], Step [1/2], Loss: 2.6427\n",
            "Epoch [85/100], Step [1/2], Loss: 2.6407\n",
            "Epoch [86/100], Step [1/2], Loss: 2.6365\n",
            "Epoch [87/100], Step [1/2], Loss: 2.6365\n",
            "Epoch [88/100], Step [1/2], Loss: 2.6323\n",
            "Epoch [89/100], Step [1/2], Loss: 2.6335\n",
            "Epoch [90/100], Step [1/2], Loss: 2.6309\n",
            "Epoch [91/100], Step [1/2], Loss: 2.6334\n",
            "Epoch [92/100], Step [1/2], Loss: 2.6311\n",
            "Epoch [93/100], Step [1/2], Loss: 2.6269\n",
            "Epoch [94/100], Step [1/2], Loss: 2.6286\n",
            "Epoch [95/100], Step [1/2], Loss: 2.6264\n",
            "Epoch [96/100], Step [1/2], Loss: 2.6231\n",
            "Epoch [97/100], Step [1/2], Loss: 2.6233\n",
            "Epoch [98/100], Step [1/2], Loss: 2.6216\n",
            "Epoch [99/100], Step [1/2], Loss: 2.6193\n",
            "Epoch [100/100], Step [1/2], Loss: 2.6244\n",
            "input:  torch.Size([100, 2264, 1, 700])  output:  torch.Size([100, 2264, 20])\n",
            "1113.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 49.16% (49.16%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8192\n",
        "dt = 50_000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7sgyh4B4Pc2",
        "outputId": "f6ffdda6-a745-4ef0-d316-fac65716ac88"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 8156, 1, 700]) torch.Size([8156]) tensor([ 9, 16,  3,  ..., 17,  5, 19])\n",
            "Epoch [1/100], Step [1/1], Loss: 3.0150\n",
            "Epoch [2/100], Step [1/1], Loss: 2.9980\n",
            "Epoch [3/100], Step [1/1], Loss: 2.9966\n",
            "Epoch [4/100], Step [1/1], Loss: 2.9948\n",
            "Epoch [5/100], Step [1/1], Loss: 2.9901\n",
            "Epoch [6/100], Step [1/1], Loss: 2.9817\n",
            "Epoch [7/100], Step [1/1], Loss: 2.9744\n",
            "Epoch [8/100], Step [1/1], Loss: 2.9668\n",
            "Epoch [9/100], Step [1/1], Loss: 2.9584\n",
            "Epoch [10/100], Step [1/1], Loss: 2.9504\n",
            "Epoch [11/100], Step [1/1], Loss: 2.9392\n",
            "Epoch [12/100], Step [1/1], Loss: 2.9243\n",
            "Epoch [13/100], Step [1/1], Loss: 2.9096\n",
            "Epoch [14/100], Step [1/1], Loss: 2.8958\n",
            "Epoch [15/100], Step [1/1], Loss: 2.8817\n",
            "Epoch [16/100], Step [1/1], Loss: 2.8671\n",
            "Epoch [17/100], Step [1/1], Loss: 2.8529\n",
            "Epoch [18/100], Step [1/1], Loss: 2.8391\n",
            "Epoch [19/100], Step [1/1], Loss: 2.8261\n",
            "Epoch [20/100], Step [1/1], Loss: 2.8130\n",
            "Epoch [21/100], Step [1/1], Loss: 2.7991\n",
            "Epoch [22/100], Step [1/1], Loss: 2.7866\n",
            "Epoch [23/100], Step [1/1], Loss: 2.7738\n",
            "Epoch [24/100], Step [1/1], Loss: 2.7598\n",
            "Epoch [25/100], Step [1/1], Loss: 2.7436\n",
            "Epoch [26/100], Step [1/1], Loss: 2.7297\n",
            "Epoch [27/100], Step [1/1], Loss: 2.7165\n",
            "Epoch [28/100], Step [1/1], Loss: 2.7034\n",
            "Epoch [29/100], Step [1/1], Loss: 2.6897\n",
            "Epoch [30/100], Step [1/1], Loss: 2.6778\n",
            "Epoch [31/100], Step [1/1], Loss: 2.6688\n",
            "Epoch [32/100], Step [1/1], Loss: 2.6596\n",
            "Epoch [33/100], Step [1/1], Loss: 2.6508\n",
            "Epoch [34/100], Step [1/1], Loss: 2.6427\n",
            "Epoch [35/100], Step [1/1], Loss: 2.6348\n",
            "Epoch [36/100], Step [1/1], Loss: 2.6266\n",
            "Epoch [37/100], Step [1/1], Loss: 2.6204\n",
            "Epoch [38/100], Step [1/1], Loss: 2.6141\n",
            "Epoch [39/100], Step [1/1], Loss: 2.6084\n",
            "Epoch [40/100], Step [1/1], Loss: 2.6023\n",
            "Epoch [41/100], Step [1/1], Loss: 2.5960\n",
            "Epoch [42/100], Step [1/1], Loss: 2.5905\n",
            "Epoch [43/100], Step [1/1], Loss: 2.5855\n",
            "Epoch [44/100], Step [1/1], Loss: 2.5806\n",
            "Epoch [45/100], Step [1/1], Loss: 2.5754\n",
            "Epoch [46/100], Step [1/1], Loss: 2.5706\n",
            "Epoch [47/100], Step [1/1], Loss: 2.5663\n",
            "Epoch [48/100], Step [1/1], Loss: 2.5623\n",
            "Epoch [49/100], Step [1/1], Loss: 2.5581\n",
            "Epoch [50/100], Step [1/1], Loss: 2.5544\n",
            "Epoch [51/100], Step [1/1], Loss: 2.5505\n",
            "Epoch [52/100], Step [1/1], Loss: 2.5468\n",
            "Epoch [53/100], Step [1/1], Loss: 2.5440\n",
            "Epoch [54/100], Step [1/1], Loss: 2.5405\n",
            "Epoch [55/100], Step [1/1], Loss: 2.5370\n",
            "Epoch [56/100], Step [1/1], Loss: 2.5342\n",
            "Epoch [57/100], Step [1/1], Loss: 2.5314\n",
            "Epoch [58/100], Step [1/1], Loss: 2.5280\n",
            "Epoch [59/100], Step [1/1], Loss: 2.5254\n",
            "Epoch [60/100], Step [1/1], Loss: 2.5224\n",
            "Epoch [61/100], Step [1/1], Loss: 2.5198\n",
            "Epoch [62/100], Step [1/1], Loss: 2.5170\n",
            "Epoch [63/100], Step [1/1], Loss: 2.5146\n",
            "Epoch [64/100], Step [1/1], Loss: 2.5121\n",
            "Epoch [65/100], Step [1/1], Loss: 2.5098\n",
            "Epoch [66/100], Step [1/1], Loss: 2.5073\n",
            "Epoch [67/100], Step [1/1], Loss: 2.5046\n",
            "Epoch [68/100], Step [1/1], Loss: 2.5025\n",
            "Epoch [69/100], Step [1/1], Loss: 2.5002\n",
            "Epoch [70/100], Step [1/1], Loss: 2.4976\n",
            "Epoch [71/100], Step [1/1], Loss: 2.4950\n",
            "Epoch [72/100], Step [1/1], Loss: 2.4930\n",
            "Epoch [73/100], Step [1/1], Loss: 2.4916\n",
            "Epoch [74/100], Step [1/1], Loss: 2.4897\n",
            "Epoch [75/100], Step [1/1], Loss: 2.4876\n",
            "Epoch [76/100], Step [1/1], Loss: 2.4858\n",
            "Epoch [77/100], Step [1/1], Loss: 2.4840\n",
            "Epoch [78/100], Step [1/1], Loss: 2.4822\n",
            "Epoch [79/100], Step [1/1], Loss: 2.4803\n",
            "Epoch [80/100], Step [1/1], Loss: 2.4786\n",
            "Epoch [81/100], Step [1/1], Loss: 2.4763\n",
            "Epoch [82/100], Step [1/1], Loss: 2.4745\n",
            "Epoch [83/100], Step [1/1], Loss: 2.4731\n",
            "Epoch [84/100], Step [1/1], Loss: 2.4715\n",
            "Epoch [85/100], Step [1/1], Loss: 2.4697\n",
            "Epoch [86/100], Step [1/1], Loss: 2.4681\n",
            "Epoch [87/100], Step [1/1], Loss: 2.4662\n",
            "Epoch [88/100], Step [1/1], Loss: 2.4647\n",
            "Epoch [89/100], Step [1/1], Loss: 2.4629\n",
            "Epoch [90/100], Step [1/1], Loss: 2.4616\n",
            "Epoch [91/100], Step [1/1], Loss: 2.4600\n",
            "Epoch [92/100], Step [1/1], Loss: 2.4583\n",
            "Epoch [93/100], Step [1/1], Loss: 2.4563\n",
            "Epoch [94/100], Step [1/1], Loss: 2.4548\n",
            "Epoch [95/100], Step [1/1], Loss: 2.4534\n",
            "Epoch [96/100], Step [1/1], Loss: 2.4521\n",
            "Epoch [97/100], Step [1/1], Loss: 2.4505\n",
            "Epoch [98/100], Step [1/1], Loss: 2.4492\n",
            "Epoch [99/100], Step [1/1], Loss: 2.4477\n",
            "Epoch [100/100], Step [1/1], Loss: 2.4463\n",
            "input:  torch.Size([20, 2264, 1, 700])  output:  torch.Size([20, 2264, 20])\n",
            "1189.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 52.52% (52.52%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8192\n",
        "dt = 50_000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.003\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpuoxPWNIGj5",
        "outputId": "b2d2b8a4-3225-4ff4-eaa0-96978943caa4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 8156, 1, 700]) torch.Size([8156]) tensor([11,  3, 11,  ...,  7,  4,  2])\n",
            "Epoch [1/100], Step [1/1], Loss: 3.0242\n",
            "Epoch [2/100], Step [1/1], Loss: 3.0057\n",
            "Epoch [3/100], Step [1/1], Loss: 2.9977\n",
            "Epoch [4/100], Step [1/1], Loss: 2.9890\n",
            "Epoch [5/100], Step [1/1], Loss: 2.9893\n",
            "Epoch [6/100], Step [1/1], Loss: 2.9845\n",
            "Epoch [7/100], Step [1/1], Loss: 2.9751\n",
            "Epoch [8/100], Step [1/1], Loss: 2.9626\n",
            "Epoch [9/100], Step [1/1], Loss: 2.9523\n",
            "Epoch [10/100], Step [1/1], Loss: 2.9451\n",
            "Epoch [11/100], Step [1/1], Loss: 2.9319\n",
            "Epoch [12/100], Step [1/1], Loss: 2.9169\n",
            "Epoch [13/100], Step [1/1], Loss: 2.9008\n",
            "Epoch [14/100], Step [1/1], Loss: 2.8863\n",
            "Epoch [15/100], Step [1/1], Loss: 2.8715\n",
            "Epoch [16/100], Step [1/1], Loss: 2.8552\n",
            "Epoch [17/100], Step [1/1], Loss: 2.8387\n",
            "Epoch [18/100], Step [1/1], Loss: 2.8227\n",
            "Epoch [19/100], Step [1/1], Loss: 2.8086\n",
            "Epoch [20/100], Step [1/1], Loss: 2.7953\n",
            "Epoch [21/100], Step [1/1], Loss: 2.7812\n",
            "Epoch [22/100], Step [1/1], Loss: 2.7638\n",
            "Epoch [23/100], Step [1/1], Loss: 2.7426\n",
            "Epoch [24/100], Step [1/1], Loss: 2.7258\n",
            "Epoch [25/100], Step [1/1], Loss: 2.7122\n",
            "Epoch [26/100], Step [1/1], Loss: 2.6960\n",
            "Epoch [27/100], Step [1/1], Loss: 2.6798\n",
            "Epoch [28/100], Step [1/1], Loss: 2.6644\n",
            "Epoch [29/100], Step [1/1], Loss: 2.6505\n",
            "Epoch [30/100], Step [1/1], Loss: 2.6337\n",
            "Epoch [31/100], Step [1/1], Loss: 2.6187\n",
            "Epoch [32/100], Step [1/1], Loss: 2.6110\n",
            "Epoch [33/100], Step [1/1], Loss: 2.6051\n",
            "Epoch [34/100], Step [1/1], Loss: 2.5943\n",
            "Epoch [35/100], Step [1/1], Loss: 2.5837\n",
            "Epoch [36/100], Step [1/1], Loss: 2.5783\n",
            "Epoch [37/100], Step [1/1], Loss: 2.5708\n",
            "Epoch [38/100], Step [1/1], Loss: 2.5662\n",
            "Epoch [39/100], Step [1/1], Loss: 2.5613\n",
            "Epoch [40/100], Step [1/1], Loss: 2.5547\n",
            "Epoch [41/100], Step [1/1], Loss: 2.5480\n",
            "Epoch [42/100], Step [1/1], Loss: 2.5423\n",
            "Epoch [43/100], Step [1/1], Loss: 2.5379\n",
            "Epoch [44/100], Step [1/1], Loss: 2.5342\n",
            "Epoch [45/100], Step [1/1], Loss: 2.5303\n",
            "Epoch [46/100], Step [1/1], Loss: 2.5266\n",
            "Epoch [47/100], Step [1/1], Loss: 2.5223\n",
            "Epoch [48/100], Step [1/1], Loss: 2.5172\n",
            "Epoch [49/100], Step [1/1], Loss: 2.5138\n",
            "Epoch [50/100], Step [1/1], Loss: 2.5112\n",
            "Epoch [51/100], Step [1/1], Loss: 2.5076\n",
            "Epoch [52/100], Step [1/1], Loss: 2.5048\n",
            "Epoch [53/100], Step [1/1], Loss: 2.5017\n",
            "Epoch [54/100], Step [1/1], Loss: 2.4993\n",
            "Epoch [55/100], Step [1/1], Loss: 2.4958\n",
            "Epoch [56/100], Step [1/1], Loss: 2.4929\n",
            "Epoch [57/100], Step [1/1], Loss: 2.4908\n",
            "Epoch [58/100], Step [1/1], Loss: 2.4876\n",
            "Epoch [59/100], Step [1/1], Loss: 2.4863\n",
            "Epoch [60/100], Step [1/1], Loss: 2.4831\n",
            "Epoch [61/100], Step [1/1], Loss: 2.4811\n",
            "Epoch [62/100], Step [1/1], Loss: 2.4788\n",
            "Epoch [63/100], Step [1/1], Loss: 2.4757\n",
            "Epoch [64/100], Step [1/1], Loss: 2.4742\n",
            "Epoch [65/100], Step [1/1], Loss: 2.4706\n",
            "Epoch [66/100], Step [1/1], Loss: 2.4685\n",
            "Epoch [67/100], Step [1/1], Loss: 2.4661\n",
            "Epoch [68/100], Step [1/1], Loss: 2.4643\n",
            "Epoch [69/100], Step [1/1], Loss: 2.4628\n",
            "Epoch [70/100], Step [1/1], Loss: 2.4594\n",
            "Epoch [71/100], Step [1/1], Loss: 2.4581\n",
            "Epoch [72/100], Step [1/1], Loss: 2.4566\n",
            "Epoch [73/100], Step [1/1], Loss: 2.4545\n",
            "Epoch [74/100], Step [1/1], Loss: 2.4525\n",
            "Epoch [75/100], Step [1/1], Loss: 2.4482\n",
            "Epoch [76/100], Step [1/1], Loss: 2.4445\n",
            "Epoch [77/100], Step [1/1], Loss: 2.4422\n",
            "Epoch [78/100], Step [1/1], Loss: 2.4394\n",
            "Epoch [79/100], Step [1/1], Loss: 2.4374\n",
            "Epoch [80/100], Step [1/1], Loss: 2.4343\n",
            "Epoch [81/100], Step [1/1], Loss: 2.4308\n",
            "Epoch [82/100], Step [1/1], Loss: 2.4276\n",
            "Epoch [83/100], Step [1/1], Loss: 2.4259\n",
            "Epoch [84/100], Step [1/1], Loss: 2.4228\n",
            "Epoch [85/100], Step [1/1], Loss: 2.4204\n",
            "Epoch [86/100], Step [1/1], Loss: 2.4184\n",
            "Epoch [87/100], Step [1/1], Loss: 2.4167\n",
            "Epoch [88/100], Step [1/1], Loss: 2.4141\n",
            "Epoch [89/100], Step [1/1], Loss: 2.4120\n",
            "Epoch [90/100], Step [1/1], Loss: 2.4102\n",
            "Epoch [91/100], Step [1/1], Loss: 2.4096\n",
            "Epoch [92/100], Step [1/1], Loss: 2.4060\n",
            "Epoch [93/100], Step [1/1], Loss: 2.4038\n",
            "Epoch [94/100], Step [1/1], Loss: 2.4029\n",
            "Epoch [95/100], Step [1/1], Loss: 2.4013\n",
            "Epoch [96/100], Step [1/1], Loss: 2.3983\n",
            "Epoch [97/100], Step [1/1], Loss: 2.3960\n",
            "Epoch [98/100], Step [1/1], Loss: 2.3953\n",
            "Epoch [99/100], Step [1/1], Loss: 2.3933\n",
            "Epoch [100/100], Step [1/1], Loss: 2.3918\n",
            "input:  torch.Size([20, 2264, 1, 700])  output:  torch.Size([20, 2264, 20])\n",
            "1223.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 54.02% (54.02%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8192\n",
        "dt = 50_000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.5).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "id": "JJDLLuVk4fDU",
        "outputId": "6b0fdf21-d4f6-4e85-be30-12f7966e78ac"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 8156, 1, 700]) torch.Size([8156]) tensor([14, 13,  9,  ...,  4,  9, 18])\n",
            "Epoch [1/100], Step [1/1], Loss: 3.0005\n",
            "Epoch [2/100], Step [1/1], Loss: 2.9964\n",
            "Epoch [3/100], Step [1/1], Loss: 2.9948\n",
            "Epoch [4/100], Step [1/1], Loss: 2.9930\n",
            "Epoch [5/100], Step [1/1], Loss: 2.9895\n",
            "Epoch [6/100], Step [1/1], Loss: 2.9865\n",
            "Epoch [7/100], Step [1/1], Loss: 2.9827\n",
            "Epoch [8/100], Step [1/1], Loss: 2.9793\n",
            "Epoch [9/100], Step [1/1], Loss: 2.9759\n",
            "Epoch [10/100], Step [1/1], Loss: 2.9727\n",
            "Epoch [11/100], Step [1/1], Loss: 2.9692\n",
            "Epoch [12/100], Step [1/1], Loss: 2.9656\n",
            "Epoch [13/100], Step [1/1], Loss: 2.9614\n",
            "Epoch [14/100], Step [1/1], Loss: 2.9572\n",
            "Epoch [15/100], Step [1/1], Loss: 2.9533\n",
            "Epoch [16/100], Step [1/1], Loss: 2.9501\n",
            "Epoch [17/100], Step [1/1], Loss: 2.9467\n",
            "Epoch [18/100], Step [1/1], Loss: 2.9436\n",
            "Epoch [19/100], Step [1/1], Loss: 2.9406\n",
            "Epoch [20/100], Step [1/1], Loss: 2.9372\n",
            "Epoch [21/100], Step [1/1], Loss: 2.9334\n",
            "Epoch [22/100], Step [1/1], Loss: 2.9297\n",
            "Epoch [23/100], Step [1/1], Loss: 2.9268\n",
            "Epoch [24/100], Step [1/1], Loss: 2.9238\n",
            "Epoch [25/100], Step [1/1], Loss: 2.9202\n",
            "Epoch [26/100], Step [1/1], Loss: 2.9169\n",
            "Epoch [27/100], Step [1/1], Loss: 2.9136\n",
            "Epoch [28/100], Step [1/1], Loss: 2.9093\n",
            "Epoch [29/100], Step [1/1], Loss: 2.9052\n",
            "Epoch [30/100], Step [1/1], Loss: 2.9015\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-cbd7e98cc36a>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-d4ec794d5eb9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, trainloader, device, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mspk_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tonic/cached_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{item}_{copy}.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tonic/cached_dataset.py\u001b[0m in \u001b[0;36mload_from_disk_cache\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{name}/{index}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0m_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "dt = 1000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "6APjtUYEvwW4",
        "outputId": "7d57e92f-b773-4fee-b1d9-febaa5bd9ca4"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 4096, 1, 700]) torch.Size([4096]) tensor([14, 17,  2,  ...,  2, 10,  4])\n",
            "Epoch [1/100], Step [1/2], Loss: 2.9966\n",
            "Epoch [2/100], Step [1/2], Loss: 2.9969\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-ef29221aec30>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-d4ec794d5eb9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, trainloader, device, num_epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mspk_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tonic/cached_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{item}_{copy}.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             logging.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tonic/cached_dataset.py\u001b[0m in \u001b[0;36mload_from_disk_cache\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    219\u001b[0m                     }\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{name}/{index}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m                 \u001b[0m_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_read_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Fall back to Python read pathway below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096\n",
        "dt = 1000_000\n",
        "transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.CropTime(max=1e6), # all timestamp units in microseconds in Tonic\n",
        "                transforms.ToFrame(\n",
        "                    sensor_size=tonic.datasets.SHD.sensor_size,\n",
        "                    time_window=dt,\n",
        "                    include_incomplete=True,\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "trainset=datasets.SHD('data', transform=transform)\n",
        "testset=datasets.SHD('data', transform=transform, train=False)\n",
        "shd_trainset = DiskCachedDataset(trainset, cache_path='./cache/shd/train/'+str(dt)+'/')\n",
        "shd_testset = DiskCachedDataset(testset, cache_path='./cache/shd/test/'+str(dt)+'/')\n",
        "trainloader = DataLoader(shd_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "testloader = DataLoader(shd_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n",
        "\n",
        "for data, labels in iter(trainloader):\n",
        "  print(data.size(), labels.size(), labels)\n",
        "  break\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork().to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train(model, criterion, optimizer, trainloader, device, num_epochs)\n",
        "accuracy(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrOkfSUhvzem",
        "outputId": "8c8d08ab-2fbf-44d1-823b-665d04306e3f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4096, 1, 700]) torch.Size([4096]) tensor([11, 15, 11,  ..., 14, 16, 10])\n",
            "Epoch [1/100], Step [1/2], Loss: 2.9958\n",
            "Epoch [2/100], Step [1/2], Loss: 2.9445\n",
            "Epoch [3/100], Step [1/2], Loss: 2.8837\n",
            "Epoch [4/100], Step [1/2], Loss: 2.8394\n",
            "Epoch [5/100], Step [1/2], Loss: 2.8033\n",
            "Epoch [6/100], Step [1/2], Loss: 2.7177\n",
            "Epoch [7/100], Step [1/2], Loss: 2.6706\n",
            "Epoch [8/100], Step [1/2], Loss: 2.6339\n",
            "Epoch [9/100], Step [1/2], Loss: 2.5822\n",
            "Epoch [10/100], Step [1/2], Loss: 2.5172\n",
            "Epoch [11/100], Step [1/2], Loss: 2.5058\n",
            "Epoch [12/100], Step [1/2], Loss: 2.4916\n",
            "Epoch [13/100], Step [1/2], Loss: 2.4687\n",
            "Epoch [14/100], Step [1/2], Loss: 2.4377\n",
            "Epoch [15/100], Step [1/2], Loss: 2.4287\n",
            "Epoch [16/100], Step [1/2], Loss: 2.4187\n",
            "Epoch [17/100], Step [1/2], Loss: 2.4057\n",
            "Epoch [18/100], Step [1/2], Loss: 2.3921\n",
            "Epoch [19/100], Step [1/2], Loss: 2.3871\n",
            "Epoch [20/100], Step [1/2], Loss: 2.3818\n",
            "Epoch [21/100], Step [1/2], Loss: 2.3648\n",
            "Epoch [22/100], Step [1/2], Loss: 2.3603\n",
            "Epoch [23/100], Step [1/2], Loss: 2.3556\n",
            "Epoch [24/100], Step [1/2], Loss: 2.3499\n",
            "Epoch [25/100], Step [1/2], Loss: 2.3436\n",
            "Epoch [26/100], Step [1/2], Loss: 2.3322\n",
            "Epoch [27/100], Step [1/2], Loss: 2.3291\n",
            "Epoch [28/100], Step [1/2], Loss: 2.3263\n",
            "Epoch [29/100], Step [1/2], Loss: 2.3258\n",
            "Epoch [30/100], Step [1/2], Loss: 2.3235\n",
            "Epoch [31/100], Step [1/2], Loss: 2.3120\n",
            "Epoch [32/100], Step [1/2], Loss: 2.3131\n",
            "Epoch [33/100], Step [1/2], Loss: 2.3013\n",
            "Epoch [34/100], Step [1/2], Loss: 2.3011\n",
            "Epoch [35/100], Step [1/2], Loss: 2.2931\n",
            "Epoch [36/100], Step [1/2], Loss: 2.2984\n",
            "Epoch [37/100], Step [1/2], Loss: 2.2968\n",
            "Epoch [38/100], Step [1/2], Loss: 2.2865\n",
            "Epoch [39/100], Step [1/2], Loss: 2.2840\n",
            "Epoch [40/100], Step [1/2], Loss: 2.2797\n",
            "Epoch [41/100], Step [1/2], Loss: 2.2787\n",
            "Epoch [42/100], Step [1/2], Loss: 2.2701\n",
            "Epoch [43/100], Step [1/2], Loss: 2.2721\n",
            "Epoch [44/100], Step [1/2], Loss: 2.2658\n",
            "Epoch [45/100], Step [1/2], Loss: 2.2626\n",
            "Epoch [46/100], Step [1/2], Loss: 2.2613\n",
            "Epoch [47/100], Step [1/2], Loss: 2.2592\n",
            "Epoch [48/100], Step [1/2], Loss: 2.2570\n",
            "Epoch [49/100], Step [1/2], Loss: 2.2546\n",
            "Epoch [50/100], Step [1/2], Loss: 2.2474\n",
            "Epoch [51/100], Step [1/2], Loss: 2.2474\n",
            "Epoch [52/100], Step [1/2], Loss: 2.2454\n",
            "Epoch [53/100], Step [1/2], Loss: 2.2442\n",
            "Epoch [54/100], Step [1/2], Loss: 2.2431\n",
            "Epoch [55/100], Step [1/2], Loss: 2.2405\n",
            "Epoch [56/100], Step [1/2], Loss: 2.2385\n",
            "Epoch [57/100], Step [1/2], Loss: 2.2361\n",
            "Epoch [58/100], Step [1/2], Loss: 2.2357\n",
            "Epoch [59/100], Step [1/2], Loss: 2.2301\n",
            "Epoch [60/100], Step [1/2], Loss: 2.2321\n",
            "Epoch [61/100], Step [1/2], Loss: 2.2275\n",
            "Epoch [62/100], Step [1/2], Loss: 2.2266\n",
            "Epoch [63/100], Step [1/2], Loss: 2.2272\n",
            "Epoch [64/100], Step [1/2], Loss: 2.2203\n",
            "Epoch [65/100], Step [1/2], Loss: 2.2220\n",
            "Epoch [66/100], Step [1/2], Loss: 2.2202\n",
            "Epoch [67/100], Step [1/2], Loss: 2.2147\n",
            "Epoch [68/100], Step [1/2], Loss: 2.2165\n",
            "Epoch [69/100], Step [1/2], Loss: 2.2129\n",
            "Epoch [70/100], Step [1/2], Loss: 2.2138\n",
            "Epoch [71/100], Step [1/2], Loss: 2.2134\n",
            "Epoch [72/100], Step [1/2], Loss: 2.2077\n",
            "Epoch [73/100], Step [1/2], Loss: 2.2069\n",
            "Epoch [74/100], Step [1/2], Loss: 2.2121\n",
            "Epoch [75/100], Step [1/2], Loss: 2.2102\n",
            "Epoch [76/100], Step [1/2], Loss: 2.2064\n",
            "Epoch [77/100], Step [1/2], Loss: 2.2014\n",
            "Epoch [78/100], Step [1/2], Loss: 2.2064\n",
            "Epoch [79/100], Step [1/2], Loss: 2.1976\n",
            "Epoch [80/100], Step [1/2], Loss: 2.2029\n",
            "Epoch [81/100], Step [1/2], Loss: 2.1965\n",
            "Epoch [82/100], Step [1/2], Loss: 2.1953\n",
            "Epoch [83/100], Step [1/2], Loss: 2.1914\n",
            "Epoch [84/100], Step [1/2], Loss: 2.1890\n",
            "Epoch [85/100], Step [1/2], Loss: 2.1868\n",
            "Epoch [86/100], Step [1/2], Loss: 2.1904\n",
            "Epoch [87/100], Step [1/2], Loss: 2.1856\n",
            "Epoch [88/100], Step [1/2], Loss: 2.1866\n",
            "Epoch [89/100], Step [1/2], Loss: 2.1805\n",
            "Epoch [90/100], Step [1/2], Loss: 2.1793\n",
            "Epoch [91/100], Step [1/2], Loss: 2.1808\n",
            "Epoch [92/100], Step [1/2], Loss: 2.1771\n",
            "Epoch [93/100], Step [1/2], Loss: 2.1776\n",
            "Epoch [94/100], Step [1/2], Loss: 2.1755\n",
            "Epoch [95/100], Step [1/2], Loss: 2.1716\n",
            "Epoch [96/100], Step [1/2], Loss: 2.1719\n",
            "Epoch [97/100], Step [1/2], Loss: 2.1722\n",
            "Epoch [98/100], Step [1/2], Loss: 2.1714\n",
            "Epoch [99/100], Step [1/2], Loss: 2.1719\n",
            "Epoch [100/100], Step [1/2], Loss: 2.1686\n",
            "input:  torch.Size([1, 2264, 1, 700])  output:  torch.Size([1, 2264, 20])\n",
            "1048.0  out of  torch.Size([2264])\n",
            "Accuracy of the network on test set: 46.29% (46.29%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9LmM1euZv4E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGnQCSwLu8B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lPX8sqM5u7Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3epHlQDFpiR",
        "outputId": "b5ae567e-26bb-4ed6-c845-c2b534fe8bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [1/2], Loss: 3.0226\n",
            "Epoch [2/100], Step [1/2], Loss: 2.9884\n",
            "Epoch [3/100], Step [1/2], Loss: 2.9621\n",
            "Epoch [4/100], Step [1/2], Loss: 2.9332\n",
            "Epoch [5/100], Step [1/2], Loss: 2.9117\n",
            "Epoch [6/100], Step [1/2], Loss: 2.8852\n",
            "Epoch [7/100], Step [1/2], Loss: 2.8579\n",
            "Epoch [8/100], Step [1/2], Loss: 2.8315\n",
            "Epoch [9/100], Step [1/2], Loss: 2.8037\n",
            "Epoch [10/100], Step [1/2], Loss: 2.7737\n",
            "Epoch [11/100], Step [1/2], Loss: 2.7403\n",
            "Epoch [12/100], Step [1/2], Loss: 2.7145\n",
            "Epoch [13/100], Step [1/2], Loss: 2.6916\n",
            "Epoch [14/100], Step [1/2], Loss: 2.6697\n",
            "Epoch [15/100], Step [1/2], Loss: 2.6356\n",
            "Epoch [16/100], Step [1/2], Loss: 2.6172\n",
            "Epoch [17/100], Step [1/2], Loss: 2.5972\n",
            "Epoch [18/100], Step [1/2], Loss: 2.5868\n",
            "Epoch [19/100], Step [1/2], Loss: 2.5661\n",
            "Epoch [20/100], Step [1/2], Loss: 2.5549\n",
            "Epoch [21/100], Step [1/2], Loss: 2.5450\n",
            "Epoch [22/100], Step [1/2], Loss: 2.5344\n",
            "Epoch [23/100], Step [1/2], Loss: 2.5255\n",
            "Epoch [24/100], Step [1/2], Loss: 2.5134\n",
            "Epoch [25/100], Step [1/2], Loss: 2.5076\n",
            "Epoch [26/100], Step [1/2], Loss: 2.4995\n",
            "Epoch [27/100], Step [1/2], Loss: 2.4907\n",
            "Epoch [28/100], Step [1/2], Loss: 2.4829\n",
            "Epoch [29/100], Step [1/2], Loss: 2.4803\n",
            "Epoch [30/100], Step [1/2], Loss: 2.4719\n",
            "Epoch [31/100], Step [1/2], Loss: 2.4625\n",
            "Epoch [32/100], Step [1/2], Loss: 2.4620\n",
            "Epoch [33/100], Step [1/2], Loss: 2.4625\n",
            "Epoch [34/100], Step [1/2], Loss: 2.4549\n",
            "Epoch [35/100], Step [1/2], Loss: 2.4550\n",
            "Epoch [36/100], Step [1/2], Loss: 2.4467\n",
            "Epoch [37/100], Step [1/2], Loss: 2.4395\n",
            "Epoch [38/100], Step [1/2], Loss: 2.4392\n",
            "Epoch [39/100], Step [1/2], Loss: 2.4336\n",
            "Epoch [40/100], Step [1/2], Loss: 2.4299\n",
            "Epoch [41/100], Step [1/2], Loss: 2.4271\n",
            "Epoch [42/100], Step [1/2], Loss: 2.4265\n",
            "Epoch [43/100], Step [1/2], Loss: 2.4224\n",
            "Epoch [44/100], Step [1/2], Loss: 2.4138\n",
            "Epoch [45/100], Step [1/2], Loss: 2.4139\n",
            "Epoch [46/100], Step [1/2], Loss: 2.4126\n",
            "Epoch [47/100], Step [1/2], Loss: 2.4053\n",
            "Epoch [48/100], Step [1/2], Loss: 2.4031\n",
            "Epoch [49/100], Step [1/2], Loss: 2.4010\n",
            "Epoch [50/100], Step [1/2], Loss: 2.3958\n",
            "Epoch [51/100], Step [1/2], Loss: 2.3953\n",
            "Epoch [52/100], Step [1/2], Loss: 2.3915\n",
            "Epoch [53/100], Step [1/2], Loss: 2.3909\n",
            "Epoch [54/100], Step [1/2], Loss: 2.3853\n",
            "Epoch [55/100], Step [1/2], Loss: 2.3800\n",
            "Epoch [56/100], Step [1/2], Loss: 2.3785\n",
            "Epoch [57/100], Step [1/2], Loss: 2.3766\n",
            "Epoch [58/100], Step [1/2], Loss: 2.3712\n",
            "Epoch [59/100], Step [1/2], Loss: 2.3756\n",
            "Epoch [60/100], Step [1/2], Loss: 2.3711\n",
            "Epoch [61/100], Step [1/2], Loss: 2.3694\n",
            "Epoch [62/100], Step [1/2], Loss: 2.3674\n",
            "Epoch [63/100], Step [1/2], Loss: 2.3651\n",
            "Epoch [64/100], Step [1/2], Loss: 2.3639\n",
            "Epoch [65/100], Step [1/2], Loss: 2.3592\n",
            "Epoch [66/100], Step [1/2], Loss: 2.3574\n",
            "Epoch [67/100], Step [1/2], Loss: 2.3540\n",
            "Epoch [68/100], Step [1/2], Loss: 2.3491\n",
            "Epoch [69/100], Step [1/2], Loss: 2.3499\n",
            "Epoch [70/100], Step [1/2], Loss: 2.3488\n",
            "Epoch [71/100], Step [1/2], Loss: 2.3486\n",
            "Epoch [72/100], Step [1/2], Loss: 2.3444\n",
            "Epoch [73/100], Step [1/2], Loss: 2.3421\n",
            "Epoch [74/100], Step [1/2], Loss: 2.3419\n",
            "Epoch [75/100], Step [1/2], Loss: 2.3378\n",
            "Epoch [76/100], Step [1/2], Loss: 2.3370\n",
            "Epoch [77/100], Step [1/2], Loss: 2.3356\n",
            "Epoch [78/100], Step [1/2], Loss: 2.3334\n",
            "Epoch [79/100], Step [1/2], Loss: 2.3330\n",
            "Epoch [80/100], Step [1/2], Loss: 2.3316\n",
            "Epoch [81/100], Step [1/2], Loss: 2.3246\n",
            "Epoch [82/100], Step [1/2], Loss: 2.3257\n",
            "Epoch [83/100], Step [1/2], Loss: 2.3213\n",
            "Epoch [84/100], Step [1/2], Loss: 2.3239\n",
            "Epoch [85/100], Step [1/2], Loss: 2.3196\n",
            "Epoch [86/100], Step [1/2], Loss: 2.3141\n",
            "Epoch [87/100], Step [1/2], Loss: 2.3141\n",
            "Epoch [88/100], Step [1/2], Loss: 2.3176\n",
            "Epoch [89/100], Step [1/2], Loss: 2.3119\n",
            "Epoch [90/100], Step [1/2], Loss: 2.3128\n",
            "Epoch [91/100], Step [1/2], Loss: 2.3084\n",
            "Epoch [92/100], Step [1/2], Loss: 2.3093\n",
            "Epoch [93/100], Step [1/2], Loss: 2.3072\n",
            "Epoch [94/100], Step [1/2], Loss: 2.3062\n",
            "Epoch [95/100], Step [1/2], Loss: 2.3039\n",
            "Epoch [96/100], Step [1/2], Loss: 2.3044\n",
            "Epoch [97/100], Step [1/2], Loss: 2.3058\n",
            "Epoch [98/100], Step [1/2], Loss: 2.2991\n",
            "Epoch [99/100], Step [1/2], Loss: 2.2983\n",
            "Epoch [100/100], Step [1/2], Loss: 2.2982\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        # print(spk_rec.size(), target.size(\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "br7nEpgEFtiS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9084134-fb3d-46c3-b8ad-fdda6e292ab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on test set: 101.57%\n"
          ]
        }
      ],
      "source": [
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "vAeYhS30Ft99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4837cbd0-6756-48a9-e28a-9cb08285d980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 3.0317\n",
            "Epoch [2/30], Step [1/2], Loss: 2.9938\n",
            "Epoch [3/30], Step [1/2], Loss: 2.9762\n",
            "Epoch [4/30], Step [1/2], Loss: 2.9536\n",
            "Epoch [5/30], Step [1/2], Loss: 2.9390\n",
            "Epoch [6/30], Step [1/2], Loss: 2.9167\n",
            "Epoch [7/30], Step [1/2], Loss: 2.8875\n",
            "Epoch [8/30], Step [1/2], Loss: 2.8673\n",
            "Epoch [9/30], Step [1/2], Loss: 2.8454\n",
            "Epoch [10/30], Step [1/2], Loss: 2.8067\n",
            "Epoch [11/30], Step [1/2], Loss: 2.7812\n",
            "Epoch [12/30], Step [1/2], Loss: 2.7538\n",
            "Epoch [13/30], Step [1/2], Loss: 2.7185\n",
            "Epoch [14/30], Step [1/2], Loss: 2.6952\n",
            "Epoch [15/30], Step [1/2], Loss: 2.6819\n",
            "Epoch [16/30], Step [1/2], Loss: 2.6518\n",
            "Epoch [17/30], Step [1/2], Loss: 2.6339\n",
            "Epoch [18/30], Step [1/2], Loss: 2.6088\n",
            "Epoch [19/30], Step [1/2], Loss: 2.6068\n",
            "Epoch [20/30], Step [1/2], Loss: 2.5991\n",
            "Epoch [21/30], Step [1/2], Loss: 2.5772\n",
            "Epoch [22/30], Step [1/2], Loss: 2.5743\n",
            "Epoch [23/30], Step [1/2], Loss: 2.5697\n",
            "Epoch [24/30], Step [1/2], Loss: 2.5524\n",
            "Epoch [25/30], Step [1/2], Loss: 2.5364\n",
            "Epoch [26/30], Step [1/2], Loss: 2.5280\n",
            "Epoch [27/30], Step [1/2], Loss: 2.5096\n",
            "Epoch [28/30], Step [1/2], Loss: 2.4908\n",
            "Epoch [29/30], Step [1/2], Loss: 2.4886\n",
            "Epoch [30/30], Step [1/2], Loss: 2.4757\n",
            "Accuracy of the network on test set: 77.59%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.003\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.95).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.0005\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.95).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6J7A-y9dr1u",
        "outputId": "507863fb-4f7f-4c47-f2bd-0ff2d8ff4498"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 3.0341\n",
            "Epoch [2/30], Step [1/2], Loss: 2.9839\n",
            "Epoch [3/30], Step [1/2], Loss: 2.9718\n",
            "Epoch [4/30], Step [1/2], Loss: 2.9489\n",
            "Epoch [5/30], Step [1/2], Loss: 2.9240\n",
            "Epoch [6/30], Step [1/2], Loss: 2.8999\n",
            "Epoch [7/30], Step [1/2], Loss: 2.8769\n",
            "Epoch [8/30], Step [1/2], Loss: 2.8414\n",
            "Epoch [9/30], Step [1/2], Loss: 2.8232\n",
            "Epoch [10/30], Step [1/2], Loss: 2.7882\n",
            "Epoch [11/30], Step [1/2], Loss: 2.7662\n",
            "Epoch [12/30], Step [1/2], Loss: 2.7370\n",
            "Epoch [13/30], Step [1/2], Loss: 2.7093\n",
            "Epoch [14/30], Step [1/2], Loss: 2.6877\n",
            "Epoch [15/30], Step [1/2], Loss: 2.6767\n",
            "Epoch [16/30], Step [1/2], Loss: 2.6558\n",
            "Epoch [17/30], Step [1/2], Loss: 2.6422\n",
            "Epoch [18/30], Step [1/2], Loss: 2.6261\n",
            "Epoch [19/30], Step [1/2], Loss: 2.6110\n",
            "Epoch [20/30], Step [1/2], Loss: 2.6055\n",
            "Epoch [21/30], Step [1/2], Loss: 2.5919\n",
            "Epoch [22/30], Step [1/2], Loss: 2.5833\n",
            "Epoch [23/30], Step [1/2], Loss: 2.5738\n",
            "Epoch [24/30], Step [1/2], Loss: 2.5633\n",
            "Epoch [25/30], Step [1/2], Loss: 2.5572\n",
            "Epoch [26/30], Step [1/2], Loss: 2.5517\n",
            "Epoch [27/30], Step [1/2], Loss: 2.5426\n",
            "Epoch [28/30], Step [1/2], Loss: 2.5406\n",
            "Epoch [29/30], Step [1/2], Loss: 2.5291\n",
            "Epoch [30/30], Step [1/2], Loss: 2.5234\n",
            "Accuracy of the network on test set: 80.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.9).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0lTu3vQd1WW",
        "outputId": "2812f8c2-14ea-45bf-bcd1-4fae6f0327ac"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 3.0187\n",
            "Epoch [2/30], Step [1/2], Loss: 2.9885\n",
            "Epoch [3/30], Step [1/2], Loss: 2.9662\n",
            "Epoch [4/30], Step [1/2], Loss: 2.9358\n",
            "Epoch [5/30], Step [1/2], Loss: 2.8963\n",
            "Epoch [6/30], Step [1/2], Loss: 2.8592\n",
            "Epoch [7/30], Step [1/2], Loss: 2.8289\n",
            "Epoch [8/30], Step [1/2], Loss: 2.8007\n",
            "Epoch [9/30], Step [1/2], Loss: 2.7678\n",
            "Epoch [10/30], Step [1/2], Loss: 2.7347\n",
            "Epoch [11/30], Step [1/2], Loss: 2.7241\n",
            "Epoch [12/30], Step [1/2], Loss: 2.7032\n",
            "Epoch [13/30], Step [1/2], Loss: 2.6823\n",
            "Epoch [14/30], Step [1/2], Loss: 2.6684\n",
            "Epoch [15/30], Step [1/2], Loss: 2.6506\n",
            "Epoch [16/30], Step [1/2], Loss: 2.6399\n",
            "Epoch [17/30], Step [1/2], Loss: 2.6199\n",
            "Epoch [18/30], Step [1/2], Loss: 2.6096\n",
            "Epoch [19/30], Step [1/2], Loss: 2.5909\n",
            "Epoch [20/30], Step [1/2], Loss: 2.5799\n",
            "Epoch [21/30], Step [1/2], Loss: 2.5757\n",
            "Epoch [22/30], Step [1/2], Loss: 2.5591\n",
            "Epoch [23/30], Step [1/2], Loss: 2.5469\n",
            "Epoch [24/30], Step [1/2], Loss: 2.5399\n",
            "Epoch [25/30], Step [1/2], Loss: 2.5296\n",
            "Epoch [26/30], Step [1/2], Loss: 2.5230\n",
            "Epoch [27/30], Step [1/2], Loss: 2.5137\n",
            "Epoch [28/30], Step [1/2], Loss: 2.5082\n",
            "Epoch [29/30], Step [1/2], Loss: 2.4972\n",
            "Epoch [30/30], Step [1/2], Loss: 2.4969\n",
            "Accuracy of the network on test set: 79.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.5).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBCpkLXUd5LG",
        "outputId": "9d49aafd-33e5-46e9-f062-3a4616abc702"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 2.9961\n",
            "Epoch [2/30], Step [1/2], Loss: 2.9910\n",
            "Epoch [3/30], Step [1/2], Loss: 2.9774\n",
            "Epoch [4/30], Step [1/2], Loss: 2.9671\n",
            "Epoch [5/30], Step [1/2], Loss: 2.9537\n",
            "Epoch [6/30], Step [1/2], Loss: 2.9410\n",
            "Epoch [7/30], Step [1/2], Loss: 2.9290\n",
            "Epoch [8/30], Step [1/2], Loss: 2.9140\n",
            "Epoch [9/30], Step [1/2], Loss: 2.8987\n",
            "Epoch [10/30], Step [1/2], Loss: 2.8862\n",
            "Epoch [11/30], Step [1/2], Loss: 2.8747\n",
            "Epoch [12/30], Step [1/2], Loss: 2.8659\n",
            "Epoch [13/30], Step [1/2], Loss: 2.8511\n",
            "Epoch [14/30], Step [1/2], Loss: 2.8393\n",
            "Epoch [15/30], Step [1/2], Loss: 2.8288\n",
            "Epoch [16/30], Step [1/2], Loss: 2.8246\n",
            "Epoch [17/30], Step [1/2], Loss: 2.8146\n",
            "Epoch [18/30], Step [1/2], Loss: 2.8070\n",
            "Epoch [19/30], Step [1/2], Loss: 2.7984\n",
            "Epoch [20/30], Step [1/2], Loss: 2.7915\n",
            "Epoch [21/30], Step [1/2], Loss: 2.7840\n",
            "Epoch [22/30], Step [1/2], Loss: 2.7767\n",
            "Epoch [23/30], Step [1/2], Loss: 2.7715\n",
            "Epoch [24/30], Step [1/2], Loss: 2.7688\n",
            "Epoch [25/30], Step [1/2], Loss: 2.7625\n",
            "Epoch [26/30], Step [1/2], Loss: 2.7578\n",
            "Epoch [27/30], Step [1/2], Loss: 2.7542\n",
            "Epoch [28/30], Step [1/2], Loss: 2.7484\n",
            "Epoch [29/30], Step [1/2], Loss: 2.7430\n",
            "Epoch [30/30], Step [1/2], Loss: 2.7343\n",
            "Accuracy of the network on test set: 86.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "\n",
        "# Model, loss function, and optimizer\n",
        "model = FullyConnectedNetwork(beta=0.99).to(device)\n",
        "criterion = SF.ce_rate_loss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        loss = criterion(spk_rec, target.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
        "# Testing loop\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, target in testloader:\n",
        "        spk_rec, mem_rec = model(data.to(device))\n",
        "        correct += SF.accuracy_rate(spk_rec, target.to(device)) * batch_size\n",
        "        total += spk_rec.size(1)\n",
        "\n",
        "        # total += target.size(0)\n",
        "        # correct += (predicted == target).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the network on test set: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT5PYM05d7VM",
        "outputId": "87729202-0184-40ae-9caf-b0b4c15bd26d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Step [1/2], Loss: 3.0450\n",
            "Epoch [2/30], Step [1/2], Loss: 2.9869\n",
            "Epoch [3/30], Step [1/2], Loss: 2.9609\n",
            "Epoch [4/30], Step [1/2], Loss: 2.9250\n",
            "Epoch [5/30], Step [1/2], Loss: 2.9039\n",
            "Epoch [6/30], Step [1/2], Loss: 2.8758\n",
            "Epoch [7/30], Step [1/2], Loss: 2.8449\n",
            "Epoch [8/30], Step [1/2], Loss: 2.8153\n",
            "Epoch [9/30], Step [1/2], Loss: 2.7858\n",
            "Epoch [10/30], Step [1/2], Loss: 2.7500\n",
            "Epoch [11/30], Step [1/2], Loss: 2.7127\n",
            "Epoch [12/30], Step [1/2], Loss: 2.6903\n",
            "Epoch [13/30], Step [1/2], Loss: 2.6671\n",
            "Epoch [14/30], Step [1/2], Loss: 2.6454\n",
            "Epoch [15/30], Step [1/2], Loss: 2.6255\n",
            "Epoch [16/30], Step [1/2], Loss: 2.6037\n",
            "Epoch [17/30], Step [1/2], Loss: 2.5890\n",
            "Epoch [18/30], Step [1/2], Loss: 2.5704\n",
            "Epoch [19/30], Step [1/2], Loss: 2.5577\n",
            "Epoch [20/30], Step [1/2], Loss: 2.5428\n",
            "Epoch [21/30], Step [1/2], Loss: 2.5316\n",
            "Epoch [22/30], Step [1/2], Loss: 2.5223\n",
            "Epoch [23/30], Step [1/2], Loss: 2.5100\n",
            "Epoch [24/30], Step [1/2], Loss: 2.5043\n",
            "Epoch [25/30], Step [1/2], Loss: 2.5006\n",
            "Epoch [26/30], Step [1/2], Loss: 2.4893\n",
            "Epoch [27/30], Step [1/2], Loss: 2.4794\n",
            "Epoch [28/30], Step [1/2], Loss: 2.4732\n",
            "Epoch [29/30], Step [1/2], Loss: 2.4690\n",
            "Epoch [30/30], Step [1/2], Loss: 2.4629\n",
            "Accuracy of the network on test set: 84.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "C9KslLqUk4zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for events, labels in testloader:\n",
        "        events, labels = events.to(device), labels.to(device)\n",
        "        output, _ = model(torch.Tensor(events).float())\n",
        "\n",
        "        print (output.size())\n",
        "        print (output)\n",
        "\n",
        "        sum = torch.cumsum(output, dim=1)\n",
        "\n",
        "        predicted = torch.argmax(sum[:,-1,:], 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy_test = (correct/total)*100\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy_test:.3f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZMZQqrc1lLAX",
        "outputId": "d8b08a56-4169-4ffb-9a20-41c646af9d74"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 2264, 20])\n",
            "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [1., 0., 1.,  ..., 1., 0., 0.],\n",
            "         ...,\n",
            "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
            "         ...,\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 1., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 1., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [1., 1., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [1., 1., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [1., 0., 0.,  ..., 1., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 1., 0.,  ..., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 1., 0.,  ..., 0., 1., 1.],\n",
            "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "         [0., 1., 0.,  ..., 0., 0., 1.]]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (10) must match the size of tensor b (2264) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-60054217176b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0maccuracy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (2264) at non-singleton dimension 0"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}